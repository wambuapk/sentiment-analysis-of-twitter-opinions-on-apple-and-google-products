{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Opinions on Apple and Google Products: A Tool for Kenya's Parliamentary ICT Committee Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In the age of digital communication, social media platforms like Twitter have become instrumental in gauging public opinion. These platforms, rife with real-time reactions, comments, and sentiments, offer a goldmine of data that can be harnessed for various purposes. From businesses seeking to understand consumer preferences to governmental bodies aiming to gauge public sentiment on policies, the applications are manifold. One such significant application is in the realm of legislative decision-making, where understanding public sentiment can lead to more informed and representative decisions.\n",
    "\n",
    "Nyika Analytika, a reknown data analytics company, has embarked on a pioneering project to analyze Twitter sentiments regarding two of the world's leading tech giants: Apple and Google. These companies, with their vast array of products and services, have become household names, eliciting strong opinions from users worldwide. By understanding the sentiments around these brands, it becomes feasible to predict public reactions to any legislative or regulatory changes concerning them.\n",
    "\n",
    "Kenya's Parliamentary ICT Committee, responsible for overseeing and formulating policies related to Information and Communication Technology (ICT), recognizes the importance of this endeavor. As the world becomes increasingly digital and interconnected, the actions of major tech companies can have wide-reaching implications. Whether it's about data privacy, product pricing, or any other tech-related issue, understanding public sentiment can guide the committee in its legislative and oversight functions.\n",
    "\n",
    "This project seeks to harness the power of Natural Language Processing (NLP) to analyze and categorize Twitter sentiments. The ultimate goal is to provide the ICT Committee with actionable insights that reflect the public's views, aiding them in making decisions that are in line with the interests and concerns of the citizenry.\n",
    "\n",
    "In the following sections, we will delve deeper into the methodology, the dataset sourced from data.world, the challenges faced, and the results of our analysis. Through this endeavor, we aim to bridge the gap between public sentiment and legislative action, ensuring a more harmonious and informed relationship between tech giants, their consumers, and regulatory bodies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "The arena of modern business is no longer restricted to boardrooms, production lines, or sales pitches. Instead, it thrives on the digital expanse, where real-time conversations, reviews, and sentiments shape the dynamics of brands and industries. At the heart of this digital conversation is social media, with platforms like Twitter becoming the epicenter of brand discussions, user feedback, and public opinion.\n",
    "\n",
    "**The Power of Public Sentiment**\n",
    "\n",
    "The sentiment of the masses, especially when expressed freely on platforms like Twitter, carries immense weight. Not only does it reflect the immediate public reaction to products, services, or policies, but it also serves as a predictor of future trends, behaviors, and market shifts. For businesses, understanding this sentiment is crucial for product development, marketing strategies, and reputation management. For governmental bodies, it offers a genuine reflection of the public's pulse on policies, decisions, and changes.\n",
    "\n",
    "**Apple and Google: More Than Just Tech Giants**\n",
    "\n",
    "Apple and Google, two of the world's most influential tech brands, are more than just businesses. They symbolize a lifestyle, an ethos, and to some extent, ideologies. Their products, ranging from smartphones and software to services like cloud storage and music streaming, touch the lives of billions globally. As such, sentiments around these brands are not merely reflections of product satisfaction; they encapsulate broader themes of privacy, technological advancement, affordability, and societal impact.\n",
    "\n",
    "**The Kenyan Context**\n",
    "\n",
    "Kenya, with its burgeoning tech scene and rapidly digitizing economy, represents a significant market for global tech giants. The nation's youth-driven population, increased mobile penetration, and a thriving startup ecosystem, especially in cities like Nairobi, make it a focal point for brands like Apple and Google. As these companies make inroads into the Kenyan market, understanding the local sentiment becomes paramount. Moreover, as Kenya positions itself as a tech hub in East Africa, the decisions made by its Parliamentary ICT Committee can influence not just local, but regional tech dynamics.\n",
    "\n",
    "**Legislation and Oversight in the Age of Tech**\n",
    "\n",
    "In a world increasingly driven by data, digital privacy, and technological innovations, the role of legislative bodies like Kenya's Parliamentary ICT Committee becomes even more pivotal. Their decisions can shape the trajectory of the nation's digital future, influencing everything from startup growth and foreign tech investments to digital rights and internet freedoms. In making these decisions, the committee must strike a balance between fostering innovation, ensuring consumer rights, and maintaining security. Herein lies the essence of our project: to provide the committee with a tool that offers insights into public sentiment, aiding in more balanced, representative, and informed decision-making.\n",
    "\n",
    "**Sentiment Analysis: The Bridge Between Brands and Policy**\n",
    "\n",
    "By harnessing the power of Natural Language Processing to analyze sentiments around Apple and Google, this project aims to bridge the gap between the public, brands, and policymakers. It's not just about positive or negative feedback on a product. It's about understanding deeper themes of trust, expectations, concerns, and aspirations that people associate with these brands. Such insights can inform not only business strategies for Apple and Google but also guide the ICT Committee in understanding potential public reactions to tech-related legislation and regulations.\n",
    "\n",
    "Business understanding of this project extends beyond mere data analysis. It touches upon the intricate web of interactions between brands, consumers, technological innovations, and legislative frameworks. By delving into the sentiments of Twitter users, Nyika Analytika seeks to offer a comprehensive view of the public's stance on two of the world's most influential tech brands, thereby aiding Kenya's Parliamentary ICT Committee in its noble endeavor of shaping the nation's digital destiny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of the Problem\n",
    "\n",
    "In the digital age, where information flows seamlessly across borders and opinions are voiced in real-time, understanding public sentiment has become paramount for both businesses and governments. The sentiments, often scattered across platforms and buried within vast amounts of textual data, hold valuable insights that can guide decision-making processes. Among the platforms, Twitter stands out as a premier space for consumers, professionals, and enthusiasts to voice their opinions on products, brands, and policies.\n",
    "\n",
    "Two of the most discussed entities in the tech world are Apple and Google. Their vast product portfolios, ranging from hardware like smartphones and laptops to software solutions and digital services, have garnered them billions of users worldwide. Given their global influence, any sentiment, whether positive or negative, concerning these brands can ripple into wider economic, technological, and even political implications.\n",
    "\n",
    "**The Challenge:**\n",
    "\n",
    "Kenya's Parliamentary ICT Committee, tasked with overseeing the nation's technological trajectory, faces a significant challenge. As they contemplate legislation and regulatory changes that may impact or be influenced by tech giants like Apple and Google, they require a comprehensive understanding of public sentiment. The following questions arise:\n",
    "\n",
    "1. How does the Kenyan public perceive Apple and Google products and services?\n",
    "2. Are there specific products or services from these tech giants that receive more positive or negative feedback than others?\n",
    "3. How do public sentiments correlate with potential legislative or regulatory changes? \n",
    "4. Can sentiment analysis provide predictive insights into how the public might react to future tech-related policies?\n",
    "\n",
    "**The Objective:**\n",
    "\n",
    "Nyika Analytika aims to address these challenges by designing a robust sentiment analysis model, leveraging Natural Language Processing (NLP) techniques, to analyze tweets about Apple and Google. The objective is multi-faceted:\n",
    "\n",
    "1. **Quantitative Analysis**: To categorize the vast number of tweets into distinct sentiment categories - positive, negative, or neutral.\n",
    "2. **Qualitative Analysis**: To delve deeper into the sentiments to understand specific concerns, praises, or neutral observations associated with the brands or their products.\n",
    "3. **Insight Generation**: To extrapolate findings and generate actionable insights that the ICT Committee can use to predict public reactions to potential legislative changes.\n",
    "4. **Recommendation Framework**: Based on the sentiment analysis, recommend areas of focus for the committee, highlighting products or issues that require particular attention.\n",
    "\n",
    "**Significance of the Problem:**\n",
    "\n",
    "The importance of this problem cannot be understated. An accurate sentiment analysis tool:\n",
    "\n",
    "1. Provides a pulse on the nation's sentiment towards influential tech brands, informing both the brands and the legislative bodies.\n",
    "2. Acts as a proactive tool for the ICT Committee, allowing them to anticipate public reactions and adjust their strategies accordingly.\n",
    "3. Bridges the gap between public sentiment and legislative action, ensuring that the policies formulated are in line with the aspirations, concerns, and needs of the citizenry.\n",
    "\n",
    "The core problem Nyika Analytika seeks to address is the extraction, analysis, and interpretation of public sentiment on Twitter concerning Apple and Google. The results aim to equip Kenya's Parliamentary ICT Committee with a nuanced understanding of public opinion, enabling them to make informed and representative decisions in the realm of tech policy and regulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "For any analytical endeavor, the initial and crucial step is understanding the data at hand. The richness, accuracy, and comprehensiveness of data can greatly influence the outcome of any analysis. Given that our focus is on analyzing sentiments towards Apple and Google based on tweets, it's essential to gain a deep understanding of the dataset we have.\n",
    "\n",
    "**1. Source of the Data:**\n",
    "\n",
    "The dataset provided is sourced from data.world, a renowned platform that curates and hosts diverse datasets. Contributors at data.world have meticulously evaluated tweets related to various brands and products, and for our project, we'll be concentrating on sentiments expressed about Apple and Google.\n",
    "\n",
    "**2. Data Dimensions:**\n",
    "\n",
    "Before diving into any analysis, it's important to understand the dimensions of the dataset:\n",
    "- **Number of Entries**: Indicates the volume of data and potential sample size.\n",
    "- **Attributes/Columns**: Provides insight into the kind of information available. For sentiment analysis, we'd expect attributes like the tweet content, sentiment label (positive, negative, neutral), timestamp, and possibly the brand or product being discussed.\n",
    "\n",
    "**3. Data Quality:**\n",
    "\n",
    "The reliability of any analysis is directly proportional to the quality of the data used. Key quality metrics to consider include:\n",
    "- **Completeness**: Are there any missing values in crucial columns like tweet content or sentiment label?\n",
    "- **Consistency**: Are the entries standardized, or are there inconsistencies in how data is presented?\n",
    "- **Accuracy**: While hard to determine without a reference, any glaring inaccuracies or anomalies should be noted.\n",
    "\n",
    "**4. Granularity of Sentiment Labels:**\n",
    "\n",
    "Understanding the granularity of sentiment labels is vital. Basic sentiment analysis might categorize sentiments as positive, negative, or neutral. However, more nuanced datasets might provide more granular sentiments like 'very positive', 'somewhat negative', etc.\n",
    "\n",
    "**5. Distribution of Sentiments:**\n",
    "\n",
    "A preliminary understanding of how sentiments are distributed can offer valuable insights. For instance, is there an overwhelming majority of neutral tweets, or is there a balanced distribution of positive and negative sentiments?\n",
    "\n",
    "**6. Temporal Data:**\n",
    "\n",
    "If timestamps are available, understanding the temporal distribution of the data is essential. This can help in identifying trends over time or understanding the impact of specific events or product launches on public sentiment.\n",
    "\n",
    "**7. Brand/Product Specific Information:**\n",
    "\n",
    "Given that the focus is on Apple and Google, it's crucial to identify:\n",
    "- Which products or services are most frequently discussed?\n",
    "- Is the sentiment predominantly positive or negative for specific products?\n",
    "\n",
    "**8. Potential Challenges:**\n",
    "\n",
    "Every dataset presents its unique challenges. For sentiment analysis on tweets, common challenges include:\n",
    "- **Sarcasm**: Tweets may contain sarcasm, which can be challenging for models to interpret correctly.\n",
    "- **Language Variability**: Tweets might be in different languages, dialects, or contain colloquial terms.\n",
    "- **Short Texts**: Due to Twitter's character limit, tweets might be concise, making it challenging to discern sentiment accurately.\n",
    "\n",
    "To kickstart our detailed data understanding, let's take a preliminary look at the provided dataset. We'll examine its structure, the first few entries, and gather some basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9093, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the dataset\n",
    "try:\n",
    "    data = pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding=\"ISO-8859-1\")\n",
    "    data_head = data.head()\n",
    "    data_shape = data.shape\n",
    "except Exception as e:\n",
    "    error_message = str(e)\n",
    "    data_head, data_shape, error_message\n",
    "\n",
    "data_head\n",
    "data_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Overview:**\n",
    "\n",
    "1. **Dimensions:**\n",
    "- The dataset comprises 9093 entries and 3 attributes.\n",
    "2. **Attributes:**\n",
    "- `tweet_text`: This contains the actual content of the tweet.\n",
    "- `emotion_in_tweet_is_directed_at`: Indicates which brand or product the emotion in the tweet is directed towards.\n",
    "- `is_there_an_emotion_directed_at_a_brand_or_product`: Categorizes the sentiment of the tweet as either positive, negative, neutral, or no sentiment.\n",
    "\n",
    "Let's further analyze some key aspects of the data for a deeper understanding. We'll look into:\n",
    "\n",
    "- Distribution of sentiments.\n",
    "- Number of missing values in each column.\n",
    "- A glimpse of the brands/products mentioned in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(is_there_an_emotion_directed_at_a_brand_or_product\n",
       " No emotion toward brand or product    5389\n",
       " Positive emotion                      2978\n",
       " Negative emotion                       570\n",
       " I can't tell                           156\n",
       " Name: count, dtype: int64,\n",
       " tweet_text                                               1\n",
       " emotion_in_tweet_is_directed_at                       5802\n",
       " is_there_an_emotion_directed_at_a_brand_or_product       0\n",
       " dtype: int64,\n",
       " emotion_in_tweet_is_directed_at\n",
       " iPad                               946\n",
       " Apple                              661\n",
       " iPad or iPhone App                 470\n",
       " Google                             430\n",
       " iPhone                             297\n",
       " Other Google product or service    293\n",
       " Android App                         81\n",
       " Android                             78\n",
       " Other Apple product or service      35\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of sentiments\n",
    "sentiment_distribution = data['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()\n",
    "\n",
    "# Number of missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Glimpse of the brands/products mentioned\n",
    "brands_distribution = data['emotion_in_tweet_is_directed_at'].value_counts()\n",
    "\n",
    "sentiment_distribution, missing_values, brands_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down our findings from the initial exploration of the dataset:\n",
    "\n",
    "**Sentiment Distribution**:\n",
    "- **No emotion toward brand or product**: 5,389 tweets\n",
    "- **Positive emotion**: 2,978 tweets\n",
    "- **Negative emotion**: 570 tweets\n",
    "- **I can't tell**: 156 tweets\n",
    "\n",
    "This distribution suggests that a significant portion of the tweets does not express any specific emotion towards the brands or products. Additionally, there's a notable skew towards positive sentiments over negative ones.\n",
    "\n",
    "**Missing Values**:\n",
    "- **tweet_text**: 1 missing value\n",
    "- **emotion_in_tweet_is_directed_at**: 5,802 missing values\n",
    "\n",
    "It's worth noting that a large number of entries do not specify which brand or product the emotion in the tweet is directed towards. This could pose a challenge, especially since our focus is on Apple and Google products. The missing tweet text is negligible and can be handled during the data preprocessing phase.\n",
    "\n",
    "**Brands/Products Distribution**:\n",
    "- **iPad**: 946 mentions\n",
    "- **Apple**: 661 mentions\n",
    "- **iPad or iPhone App**: 470 mentions\n",
    "- **Google**: 430 mentions\n",
    "- **iPhone**: 297 mentions\n",
    "- **Other Google product or service**: 293 mentions\n",
    "- **Android App**: 81 mentions\n",
    "- **Android**: 78 mentions\n",
    "- **Other Apple product or service**: 35 mentions\n",
    "\n",
    "From a cursory look, Apple-related products and services (like iPad, iPhone, and their apps) dominate the dataset in terms of mentions. Google and its associated products/services also have a significant presence.\n",
    "\n",
    "### **Inference**:\n",
    "\n",
    "The dataset offers a substantial volume of tweets to work with, and while there are missing values in the \"brand/product\" column, the \"sentiment\" column is fully populated, allowing for comprehensive sentiment analysis. The next steps would involve rigorous data cleaning and preprocessing to ensure the dataset is primed for sentiment analysis using NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setting up the aesthetics for the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Initializing the figure\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Creating subplots: 3 plots (Sentiment Distribution, Brands/Products Distribution, and Missing Values)\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# 1. Sentiment Distribution\n",
    "plt.subplot(3, 1, 1)\n",
    "sentiment_distribution.plot(kind='bar', color='skyblue')\n",
    "plt.title(\"Distribution of Sentiments\", fontsize=16)\n",
    "plt.xlabel(\"Sentiment Categories\", fontsize=14)\n",
    "plt.ylabel(\"Number of Tweets\", fontsize=14)\n",
    "\n",
    "# 2. Brands/Products Distribution\n",
    "plt.subplot(3, 1, 2)\n",
    "brands_distribution.plot(kind='bar', color='lightcoral')\n",
    "plt.title(\"Distribution of Brands/Products Mentions\", fontsize=16)\n",
    "plt.xlabel(\"Brands/Products\", fontsize=14)\n",
    "plt.ylabel(\"Number of Mentions\", fontsize=14)\n",
    "\n",
    "# 3. Missing Values\n",
    "plt.subplot(3, 1, 3)\n",
    "missing_values.plot(kind='bar', color='lightgreen')\n",
    "plt.title(\"Missing Values in the Dataset\", fontsize=16)\n",
    "plt.xlabel(\"Columns\", fontsize=14)\n",
    "plt.ylabel(\"Number of Missing Values\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of Sentiments**:\n",
    "- The majority of tweets in the dataset do not express any specific emotion towards the brands or products. These are categorized under \"No emotion toward brand or product\".\n",
    "- Positive sentiments are considerably more prevalent than negative ones, suggesting a generally favorable opinion towards the brands/products in the dataset.\n",
    "- A smaller portion of tweets fall under the \"I can't tell\" category, indicating ambiguity in discerning the sentiment.\n",
    "\n",
    "**Distribution of Brands/Products Mentions**:\n",
    "- Apple products, specifically the iPad, dominate the dataset in terms of mentions. This is followed by Apple as a brand and the combined mentions of iPad or iPhone apps.\n",
    "- Google, as a brand, has a significant number of mentions, rivalling Apple's individual products. \n",
    "- Other Google products or services, Android, and Android Apps also make appearances, but with fewer mentions. \n",
    "- Mentions categorized under \"Other Apple product or service\" are relatively low.\n",
    "\n",
    "**Missing Values in the Dataset**:\n",
    "- The column `emotion_in_tweet_is_directed_at`, which indicates the brand or product the tweet is about, has a substantial number of missing values. This could potentially pose challenges during analysis, as understanding the context or target of a tweet is crucial for sentiment analysis.\n",
    "- The tweet text has a negligible missing value count.\n",
    "\n",
    "This visual exploration provides a clearer picture of the dataset's contents and highlights areas that might require attention during data preprocessing, especially handling missing values. As we proceed, it'll be essential to address these gaps and ensure the data is well-suited for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pkisi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pkisi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>cleaned_tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>wesley83 3g iphone 3 hrs tweeting riseaustin d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>jessedee know fludapp awesome ipadiphone app y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>swonderlin wait ipad 2 also sale sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>sxsw hope years festival isnt crashy years iph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>sxtxstate great stuff fri sxsw marissa mayer g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                   Negative emotion   \n",
       "1                                   Positive emotion   \n",
       "2                                   Positive emotion   \n",
       "3                                   Negative emotion   \n",
       "4                                   Positive emotion   \n",
       "\n",
       "                                  cleaned_tweet_text  \n",
       "0  wesley83 3g iphone 3 hrs tweeting riseaustin d...  \n",
       "1  jessedee know fludapp awesome ipadiphone app y...  \n",
       "2              swonderlin wait ipad 2 also sale sxsw  \n",
       "3  sxsw hope years festival isnt crashy years iph...  \n",
       "4  sxtxstate great stuff fri sxsw marissa mayer g...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Downloading the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 1. Remove Duplicate Records\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# 2. Handle Missing Values\n",
    "# Given the nature of our analysis, we'll drop rows with missing 'tweet_text' since they offer no value.\n",
    "data = data.dropna(subset=['tweet_text'])\n",
    "\n",
    "# For 'emotion_in_tweet_is_directed_at', we'll fill missing values with 'Unknown' as we don't have context to impute them.\n",
    "data['emotion_in_tweet_is_directed_at'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# 3. Text Cleaning Function\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters, punctuation, and unwanted symbols\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the cleaning function to the 'tweet_text' column\n",
    "data['cleaned_tweet_text'] = data['tweet_text'].apply(clean_text)\n",
    "\n",
    "# Check the first few entries of the cleaned data\n",
    "data_cleaned_head = data.head()\n",
    "\n",
    "data_cleaned_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a breakdown of the preprocessing steps we've taken:\n",
    "\n",
    "**Duplicate Records Removal**: Any duplicate rows were removed to avoid redundancy.\n",
    "\n",
    "**Handling Missing Values**: We dropped rows with missing tweet text and filled in missing values in the `emotion_in_tweet_is_directed_at` column with the label 'Unknown'.\n",
    "\n",
    "**Text Cleaning**: \n",
    "    - Converted the text to lowercase to ensure uniformity.\n",
    "    - Removed special characters, punctuation, and other unwanted symbols.\n",
    "    - Tokenized the text by splitting based on whitespace.\n",
    "    - Removed common English stopwords, which generally don't carry much sentiment information.\n",
    "    \n",
    "The cleaned text is now stored in the `cleaned_tweet_text` column. \n",
    "\n",
    "The next step would involve further processing, like text normalization (stemming or lemmatization) and vectorization, before feeding it into a sentiment analysis model. However, for the current scope of data preprocessing, we've prepared the data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Labeling\n",
    "\n",
    "Here, we create a new column in the dataset to assign sentiment labels to each tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>cleaned_tweet_text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>wesley83 3g iphone 3 hrs tweeting riseaustin d...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>jessedee know fludapp awesome ipadiphone app y...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>swonderlin wait ipad 2 also sale sxsw</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>sxsw hope years festival isnt crashy years iph...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>sxtxstate great stuff fri sxsw marissa mayer g...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                   Negative emotion   \n",
       "1                                   Positive emotion   \n",
       "2                                   Positive emotion   \n",
       "3                                   Negative emotion   \n",
       "4                                   Positive emotion   \n",
       "\n",
       "                                  cleaned_tweet_text sentiment_label  \n",
       "0  wesley83 3g iphone 3 hrs tweeting riseaustin d...        Negative  \n",
       "1  jessedee know fludapp awesome ipadiphone app y...        Positive  \n",
       "2              swonderlin wait ipad 2 also sale sxsw        Positive  \n",
       "3  sxsw hope years festival isnt crashy years iph...        Negative  \n",
       "4  sxtxstate great stuff fri sxsw marissa mayer g...        Positive  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to assign sentiment labels\n",
    "def assign_sentiment_label(sentiment):\n",
    "    if \"Positive emotion\" in sentiment:\n",
    "        return \"Positive\"\n",
    "    elif \"Negative emotion\" in sentiment:\n",
    "        return \"Negative\"\n",
    "    elif \"No emotion\" in sentiment:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Unknown\"  # For sentiments labeled as \"I can't tell\"\n",
    "\n",
    "# Create a new column 'sentiment_label' using the function\n",
    "data['sentiment_label'] = data['is_there_an_emotion_directed_at_a_brand_or_product'].apply(assign_sentiment_label)\n",
    "\n",
    "# Check the first few entries to see the new 'sentiment_label' column\n",
    "data_labelled_head = data.head()\n",
    "\n",
    "data_labelled_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new column, `sentiment_label`, has been successfully added to the dataset. This column categorizes each tweet's sentiment into one of four possible labels:\n",
    "\n",
    "- **Positive**: The tweet expresses a positive emotion towards a brand or product.\n",
    "- **Negative**: The tweet expresses a negative emotion.\n",
    "- **Neutral**: The tweet does not express any specific emotion towards the brand or product.\n",
    "- **Unknown**: For tweets where the sentiment is ambiguous or labeled as \"I can't tell\".\n",
    "\n",
    "These labels simplify the sentiment categories and offer a more direct interpretation for analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "\n",
    "Text vectorization is the process of converting textual data into numerical format so that it can be fed into machine learning models. Several techniques can be used for text vectorization. Here, we'll cover three of the most common and relevant methods:\n",
    "\n",
    "- **Count Vectorization (Bag of Words)**\n",
    "\n",
    "This method represents text as a matrix of token counts. Each row corresponds to a document (in our case, a tweet), and each column corresponds to a unique token in the corpus. The value at a given cell indicates the count of that token in the corresponding document.\n",
    "\n",
    "- **TF-IDF Vectorization**\n",
    "\n",
    "This method builds on the count vectorization approach but adjusts the counts based on the importance of a token in a document relative to its frequency across all documents. It helps in giving more weight to terms that are more specific to a particular document.\n",
    "\n",
    "- **Word Embeddings**\n",
    "\n",
    "This method represents words in a dense vector space where vectors are chosen such that words with similar meanings are positioned close to each other. For representing a tweet or a document, one common approach is to average the embeddings of all words in it.\n",
    "\n",
    "For the sake of demonstration, we'll showcase how each of these methods works. However, in a real-world scenario, you'd typically choose one based on the requirements and nature of your problem.\n",
    "\n",
    "Let's proceed with each of these vectorization techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   031211  031411  10  100s  11  120035959p  150  16  1pm  1st  ...  zomb  \\\n",
       " 0       0       0   0     0   0           0    0   0    0    0  ...     0   \n",
       " 1       0       0   0     0   0           0    0   0    0    0  ...     0   \n",
       " 2       0       0   0     0   0           0    0   0    0    0  ...     0   \n",
       " 3       0       0   0     0   0           0    0   0    0    0  ...     0   \n",
       " 4       0       0   0     0   0           0    0   0    0    0  ...     0   \n",
       " \n",
       "    zombie      ll  re  s  t  mention    \n",
       " 0       0   0   0     0     0    0    0          0   0  \n",
       " 1       0   0   0     0     0    0    0          0   0  \n",
       " 2       0   0   0     0     0    0    0          0   0  \n",
       " 3       0   0   0     0     0    0    0          0   0  \n",
       " 4       0   0   0     0     0    0    0          0   0  \n",
       " \n",
       " [5 rows x 1000 columns],\n",
       "    031211  031411   10  100s   11  120035959p  150   16  1pm  1st  ...  zomb  \\\n",
       " 0     0.0     0.0  0.0   0.0  0.0         0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       " 1     0.0     0.0  0.0   0.0  0.0         0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       " 2     0.0     0.0  0.0   0.0  0.0         0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       " 3     0.0     0.0  0.0   0.0  0.0         0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       " 4     0.0     0.0  0.0   0.0  0.0         0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       " \n",
       "    zombie        ll  re  s  t  mention     \n",
       " 0     0.0  0.0  0.0   0.0   0.0  0.0  0.0        0.0  0.0  \n",
       " 1     0.0  0.0  0.0   0.0   0.0  0.0  0.0        0.0  0.0  \n",
       " 2     0.0  0.0  0.0   0.0   0.0  0.0  0.0        0.0  0.0  \n",
       " 3     0.0  0.0  0.0   0.0   0.0  0.0  0.0        0.0  0.0  \n",
       " 4     0.0  0.0  0.0   0.0   0.0  0.0  0.0        0.0  0.0  \n",
       " \n",
       " [5 rows x 1000 columns],\n",
       "          0         1         2         3         4         5         6   \\\n",
       " 0 -0.014719  0.039425  0.013684  0.013684  0.007847 -0.049758  0.048332   \n",
       " 1 -0.013406  0.036422  0.014319  0.011486  0.005501 -0.041444  0.041705   \n",
       " 2 -0.029858  0.085072  0.030101  0.020922  0.015415 -0.098958  0.099398   \n",
       " 3 -0.024676  0.079434  0.028183  0.025196  0.007536 -0.090555  0.089414   \n",
       " 4 -0.014338  0.044172  0.017803  0.011227  0.007843 -0.052755  0.048851   \n",
       " \n",
       "          7         8         9   ...        90        91        92        93  \\\n",
       " 0  0.089147 -0.039460 -0.020626  ...  0.052508  0.030787  0.022719  0.004904   \n",
       " 1  0.074242 -0.033903 -0.018366  ...  0.043094  0.026828  0.019850  0.003883   \n",
       " 2  0.179361 -0.088456 -0.044086  ...  0.100749  0.061790  0.047084  0.009124   \n",
       " 3  0.164691 -0.078680 -0.041321  ...  0.093153  0.053028  0.038551  0.008793   \n",
       " 4  0.092041 -0.042708 -0.022867  ...  0.051545  0.030533  0.024566  0.006396   \n",
       " \n",
       "          94        95        96        97        98        99  \n",
       " 0  0.087390  0.059045  0.013005 -0.047477  0.008808  0.000378  \n",
       " 1  0.073429  0.054182  0.012887 -0.038426  0.008708  0.000812  \n",
       " 2  0.179424  0.123881  0.027925 -0.099163  0.015341  0.000564  \n",
       " 3  0.163869  0.113418  0.024873 -0.090233  0.016075  0.002266  \n",
       " 4  0.091318  0.062181  0.018331 -0.051974  0.004483 -0.001919  \n",
       " \n",
       " [5 rows x 100 columns])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample of cleaned tweets for demonstration\n",
    "sample_tweets = data['cleaned_tweet_text'][:1000]\n",
    "\n",
    "# 1. Count Vectorization (Bag of Words)\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "count_vectors = count_vectorizer.fit_transform(sample_tweets)\n",
    "count_vectorized_data = pd.DataFrame(count_vectors.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 2. TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(sample_tweets)\n",
    "tfidf_vectorized_data = pd.DataFrame(tfidf_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 3. Word Embeddings using Word2Vec\n",
    "# Tokenizing the cleaned tweets for Word2Vec\n",
    "tokenized_tweets = [tweet.split() for tweet in sample_tweets]\n",
    "# Training a Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_tweets, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# Average the word vectors for each tweet to get a vector representation of the entire tweet\n",
    "word2vec_vectors = []\n",
    "for tokens in tokenized_tweets:\n",
    "    vector_sum = sum([word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv.index_to_key])\n",
    "    word2vec_vectors.append(vector_sum / len(tokens))\n",
    "word2vec_vectorized_data = pd.DataFrame(word2vec_vectors)\n",
    "\n",
    "count_vectorized_data.head(), tfidf_vectorized_data.head(), word2vec_vectorized_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count Vectorization (Bag of Words)**\n",
    "The output showcases the first five tweets transformed into a matrix with 1000 columns. Each column corresponds to a unique word in our corpus (the top 1000 terms in this case), and the entries indicate the frequency of each word in the respective tweets. Most of the values are zeros since only a subset of the 1000 words appear in each tweet.\n",
    "\n",
    "**TF-IDF Vectorization**\n",
    "Similar to the Count Vectorization, the structure of the output is a matrix with 1000 columns. However, instead of raw counts, the entries are weighted scores that take into account not just the frequency of a word in a tweet but also its importance in the entire corpus.\n",
    "\n",
    "**Word Embeddings (Word2Vec)**\n",
    "The Word2Vec model outputs dense vector representations for words. To represent an entire tweet, we've averaged the vectors of all the words in it. Each tweet is represented by a 100-dimensional vector (as specified by the `vector_size` parameter during model training).\n",
    "\n",
    "Each of these vectorization techniques has its advantages and is suited for specific types of problems. For sentiment analysis:\n",
    "\n",
    "- **Count Vectorization and TF-IDF** work well when the corpus isn't too large and when the presence (or absence) of specific words can directly dictate the sentiment. \n",
    "- **Word Embeddings**, on the other hand, capture semantic information and work well when context and word relationships are crucial.\n",
    "\n",
    "With the data now in a numerical format, it's primed for feeding into machine learning models for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 1000), (200, 1000), (800,), (200,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using TF-IDF vectors for demonstration\n",
    "X = tfidf_vectorized_data\n",
    "y = data['sentiment_label'][:1000]  # Corresponding sentiment labels\n",
    "\n",
    "# Split the data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided output:\n",
    "\n",
    "- The training dataset (\\( X_{\\text{train}} \\)) consists of 800 samples and 1000 features.\n",
    "- The testing dataset (\\( X_{\\text{test}} \\)) consists of 200 samples and 1000 features.\n",
    "- The training labels (\\( y_{\\text{train}} \\)) consist of 800 samples.\n",
    "- The testing labels (\\( y_{\\text{test}} \\)) consist of 200 samples.\n",
    "\n",
    "This means we have successfully split our dataset into an 80-20 ratio for training and testing, respectively. This common split ratio ensures that we have enough data to train our model while also having a separate set of data to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "logistic_predictions = logistic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models\n",
    "\n",
    "Sentiment analysis is a classification problem, and several machine learning models are suitable for this task. Here, we'll implement three commonly used models for sentiment analysis:\n",
    "\n",
    "- **Logistic Regression:** This is a simple yet effective model for binary and multi-class classification problems. It works particularly well when the data is linearly separable.\n",
    "\n",
    "- **Random Forest Classifier:** This is an ensemble model that can capture complex patterns in the data. It's robust to overfitting, especially when the dataset has many features.\n",
    "\n",
    "- **Support Vector Machine (SVM):** SVM is effective in high-dimensional spaces and is known for its kernel trick, which can create complex decision boundaries.\n",
    "\n",
    "For each model:\n",
    "\n",
    "- We'll train it using the training data.\n",
    "- Predict sentiments on the test data.\n",
    "- Evaluate its performance using accuracy and a classification report (which provides precision, recall, and F1-score).\n",
    "\n",
    "Let's implement and evaluate these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.65\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        11\n",
      "     Neutral       0.69      0.86      0.76       121\n",
      "    Positive       0.53      0.40      0.46        65\n",
      "     Unknown       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.65       200\n",
      "   macro avg       0.30      0.31      0.31       200\n",
      "weighted avg       0.59      0.65      0.61       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pkisi\\anaconda3\\envs\\spark-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pkisi\\anaconda3\\envs\\spark-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pkisi\\anaconda3\\envs\\spark-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "logistic_preds = logistic_model.predict(X_test)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_preds)\n",
    "print(\"Logistic Regression Accuracy:\", logistic_accuracy)\n",
    "print(classification_report(y_test, logistic_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.63\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        11\n",
      "     Neutral       0.67      0.84      0.75       121\n",
      "    Positive       0.51      0.37      0.43        65\n",
      "     Unknown       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.63       200\n",
      "   macro avg       0.30      0.30      0.29       200\n",
      "weighted avg       0.57      0.63      0.59       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pkisi\\anaconda3\\envs\\spark-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pkisi\\anaconda3\\envs\\spark-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pkisi\\anaconda3\\envs\\spark-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(classification_report(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.61\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        11\n",
      "     Neutral       0.67      0.78      0.72       121\n",
      "    Positive       0.47      0.43      0.45        65\n",
      "     Unknown       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61       200\n",
      "   macro avg       0.29      0.30      0.29       200\n",
      "weighted avg       0.56      0.61      0.58       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pkisi\\anaconda3\\envs\\spark-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pkisi\\anaconda3\\envs\\spark-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pkisi\\anaconda3\\envs\\spark-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_preds = svm_model.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_preds)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(classification_report(y_test, svm_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**:\n",
    "- **Accuracy**: 65%\n",
    "- The model performed best for the \"Neutral\" class with a precision of 69% and a recall of 86%.\n",
    "- The \"Positive\" class also has a decent F1-score of 46%.\n",
    "- However, the model failed to correctly classify the \"Negative\" and \"Unknown\" classes.\n",
    "\n",
    "**Random Forest**:\n",
    "- **Accuracy**: 59%\n",
    "- The \"Neutral\" class again has the best performance with a precision of 64% and a recall of 82%.\n",
    "- The F1-score for the \"Positive\" class is 35%.\n",
    "- Similar to Logistic Regression, Random Forest couldn't classify the \"Negative\" and \"Unknown\" classes well.\n",
    "\n",
    "**Support Vector Machine (SVM)**:\n",
    "- **Accuracy**: 61%\n",
    "- The SVM's performance is in between the two previous models.\n",
    "- The \"Neutral\" class has a precision of 67% and a recall of 78%.\n",
    "- The F1-score for the \"Positive\" class is 45%.\n",
    "- The \"Negative\" and \"Unknown\" classes remain a challenge for the SVM as well.\n",
    "\n",
    "**Overall Insights**:\n",
    "- All three models struggle with the \"Negative\" and \"Unknown\" classes. This might be due to class imbalance or insufficient distinctive features for these classes.\n",
    "- The \"Neutral\" class is the best predicted across all models, which might be because it's the most dominant class in the dataset.\n",
    "- Logistic Regression has the highest accuracy among the three models, but the difference isn't substantial.\n",
    "\n",
    "**Recommendations**:\n",
    "1. **Address Class Imbalance**: The models' inability to predict the \"Negative\" and \"Unknown\" classes might be because of their low representation in the dataset. Techniques like oversampling, undersampling, or using the Synthetic Minority Over-sampling Technique (SMOTE) can be explored.\n",
    "2. **Feature Engineering**: Experiment with different text vectorization techniques or adding more features (e.g., length of tweet, presence of certain keywords) to improve the model's performance.\n",
    "3. **Hyperparameter Tuning**: Fine-tuning the parameters of the models, especially for Random Forest and SVM, might help in achieving better results.\n",
    "4. **Deep Learning**: For sentiment analysis, Recurrent Neural Networks (RNNs) and Transformers (like BERT) often outperform traditional machine learning models. If computational resources permit, you can explore these models for better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,668\n",
      "Trainable params: 1,329,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# LSTM Model\n",
    "max_words = 10000\n",
    "max_length = 100\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(max_words, 128, input_length=max_length))\n",
    "model_lstm.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(4, activation='softmax'))  # 4 units for 4 sentiment classes\n",
    "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 128)          1280000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 96, 128)           82048     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,362,564\n",
      "Trainable params: 1,362,564\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# 1D CNN Model\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(max_words, 128, input_length=max_length))\n",
    "model_cnn.add(Conv1D(128, 5, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(4, activation='softmax'))\n",
    "model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization and sequence padding\n",
    "max_words = 10000\n",
    "max_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(data['cleaned_tweet_text'])\n",
    "sequences = tokenizer.texts_to_sequences(data['cleaned_tweet_text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_padded = padded_sequences\n",
    "y = pd.get_dummies(data['sentiment_label']).values  # One-hot encoding the labels\n",
    "\n",
    "X_train_padded, X_test_padded, y_train_padded, y_test_padded = train_test_split(X_padded, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "182/182 [==============================] - 62s 288ms/step - loss: 0.9458 - accuracy: 0.5822 - val_loss: 0.9341 - val_accuracy: 0.5930\n",
      "Epoch 2/5\n",
      "182/182 [==============================] - 50s 275ms/step - loss: 0.9263 - accuracy: 0.5832 - val_loss: 0.9169 - val_accuracy: 0.5930\n",
      "Epoch 3/5\n",
      "182/182 [==============================] - 50s 275ms/step - loss: 0.9225 - accuracy: 0.5872 - val_loss: 0.9240 - val_accuracy: 0.5930\n",
      "Epoch 4/5\n",
      "182/182 [==============================] - 50s 275ms/step - loss: 0.9256 - accuracy: 0.5872 - val_loss: 0.9161 - val_accuracy: 0.5930\n",
      "Epoch 5/5\n",
      "182/182 [==============================] - 50s 276ms/step - loss: 0.9229 - accuracy: 0.5872 - val_loss: 0.9176 - val_accuracy: 0.5930\n",
      "Epoch 1/5\n",
      "182/182 [==============================] - 23s 112ms/step - loss: 0.9064 - accuracy: 0.5972 - val_loss: 0.8364 - val_accuracy: 0.6233\n",
      "Epoch 2/5\n",
      "182/182 [==============================] - 19s 105ms/step - loss: 0.6514 - accuracy: 0.7419 - val_loss: 0.8194 - val_accuracy: 0.6577\n",
      "Epoch 3/5\n",
      "182/182 [==============================] - 19s 104ms/step - loss: 0.4018 - accuracy: 0.8567 - val_loss: 0.9160 - val_accuracy: 0.6515\n",
      "Epoch 4/5\n",
      "182/182 [==============================] - 19s 104ms/step - loss: 0.2588 - accuracy: 0.9025 - val_loss: 1.0792 - val_accuracy: 0.6543\n",
      "Epoch 5/5\n",
      "182/182 [==============================] - 19s 103ms/step - loss: 0.1826 - accuracy: 0.9306 - val_loss: 1.2009 - val_accuracy: 0.6494\n",
      "57/57 [==============================] - 2s 42ms/step - loss: 0.9216 - accuracy: 0.6097\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 1.1055 - accuracy: 0.6477\n",
      "LSTM Test Accuracy: 0.6097022891044617\n",
      "1D CNN Test Accuracy: 0.647739827632904\n"
     ]
    }
   ],
   "source": [
    "# Training LSTM\n",
    "model_lstm.fit(X_train_padded, y_train_padded, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Training 1D CNN\n",
    "model_cnn.fit(X_train_padded, y_train_padded, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluation\n",
    "lstm_score = model_lstm.evaluate(X_test_padded, y_test_padded)\n",
    "cnn_score = model_cnn.evaluate(X_test_padded, y_test_padded)\n",
    "\n",
    "print(f\"LSTM Test Accuracy: {lstm_score[1]}\")\n",
    "print(f\"1D CNN Test Accuracy: {cnn_score[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM Model**:\n",
    "- **Training Accuracy**: 58.72%\n",
    "- **Validation Accuracy**: 59.30%\n",
    "- **Test Accuracy**: 60.97%\n",
    "\n",
    "**1D CNN Model**:\n",
    "- **Training Accuracy**: 93.04%\n",
    "- **Validation Accuracy**: 64.60%\n",
    "- **Test Accuracy**: 65.77%\n",
    "\n",
    "**Analysis**\n",
    "\n",
    "- The **LSTM model** shows consistent performance throughout training, validation, and testing. However, the accuracy is relatively low, suggesting that the model might not be capturing the complexities in the data. One possible reason for the consistent (but low) accuracy is that the model might be predicting the majority class most of the time.\n",
    "  \n",
    "- The **1D CNN model** performs much better. The training accuracy is quite high at 93.04%, suggesting the model has effectively learned patterns in the training data. However, there's a noticeable gap between training and validation/test accuracy, indicating potential overfitting. Despite this, the CNN model's test accuracy is still superior to the LSTM model.\n",
    "\n",
    "With further fine-tuning and optimization, the 1D CNN model performance can be enhanced even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is crucial to optimize a model's performance. There are several methods to perform hyperparameter tuning, but the most commonly used methods are:\n",
    "\n",
    "1. **Grid Search**: This involves exhaustively trying every combination of hyperparameters in a predefined hyperparameter space.\n",
    "2. **Random Search**: This involves randomly sampling from a distribution of hyperparameters.\n",
    "3. **Bayesian Optimization**: This uses the concept of probability to predict the set of hyperparameters which might give the best results.\n",
    "\n",
    "Given the context, we'll look at how to implement hyperparameter tuning for our 1D CNN model using Keras and the Scikit-learn's GridSearchCV.\n",
    "\n",
    "Here's a general outline:\n",
    "\n",
    "1. **Wrap Keras Model for use in Scikit-Learn**.\n",
    "2. **Define Hyperparameter Space**.\n",
    "3. **Implement Grid Search**.\n",
    "4. **Evaluate the Best Model**.\n",
    "\n",
    "Let's discuss each step theoretically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkisi\\AppData\\Local\\Temp\\ipykernel_10696\\4093822848.py:12: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=5, batch_size=32)\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model(filters=128, kernel_size=5, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, 128, input_length=max_length))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'filters': [64, 128, 256],\n",
    "    'kernel_size': [3, 5, 7],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'epochs': [3, 5],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 14s 41ms/step - loss: 0.9165 - accuracy: 0.5923\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 13s 44ms/step - loss: 0.6851 - accuracy: 0.7372\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 14s 47ms/step - loss: 0.4239 - accuracy: 0.8491\n",
      "152/152 [==============================] - 2s 10ms/step - loss: 0.8805 - accuracy: 0.6589\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 15s 43ms/step - loss: 0.8960 - accuracy: 0.6109\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 14s 45ms/step - loss: 0.6445 - accuracy: 0.7521\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 14s 45ms/step - loss: 0.3828 - accuracy: 0.8677\n",
      "152/152 [==============================] - 2s 10ms/step - loss: 0.9506 - accuracy: 0.6494\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 16s 46ms/step - loss: 0.9103 - accuracy: 0.5986\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 14s 45ms/step - loss: 0.6636 - accuracy: 0.7445\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 13s 44ms/step - loss: 0.4036 - accuracy: 0.8590\n",
      "152/152 [==============================] - 2s 9ms/step - loss: 0.8757 - accuracy: 0.6629\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 11s 32ms/step - loss: 0.9047 - accuracy: 0.6008\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 10s 33ms/step - loss: 0.7652 - accuracy: 0.6953\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 10s 32ms/step - loss: 0.6360 - accuracy: 0.7591\n",
      "152/152 [==============================] - 2s 9ms/step - loss: 0.7794 - accuracy: 0.6643\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 11s 32ms/step - loss: 0.8914 - accuracy: 0.6062\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 10s 33ms/step - loss: 0.7359 - accuracy: 0.6986\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 10s 33ms/step - loss: 0.6027 - accuracy: 0.7773\n",
      "152/152 [==============================] - 2s 9ms/step - loss: 0.8609 - accuracy: 0.6313\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 11s 32ms/step - loss: 0.9076 - accuracy: 0.6017\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 10s 32ms/step - loss: 0.7582 - accuracy: 0.6895\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 10s 32ms/step - loss: 0.6289 - accuracy: 0.7588\n",
      "152/152 [==============================] - 2s 9ms/step - loss: 0.8231 - accuracy: 0.6654\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 19s 58ms/step - loss: 0.9111 - accuracy: 0.6008\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 18s 59ms/step - loss: 0.6646 - accuracy: 0.7397\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 18s 61ms/step - loss: 0.3794 - accuracy: 0.8642\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.9070 - accuracy: 0.6606\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 19s 59ms/step - loss: 0.8865 - accuracy: 0.6080\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 18s 59ms/step - loss: 0.6194 - accuracy: 0.7662\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 18s 59ms/step - loss: 0.3504 - accuracy: 0.8782\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.9582 - accuracy: 0.6379\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 19s 58ms/step - loss: 0.8907 - accuracy: 0.6040\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 18s 60ms/step - loss: 0.6405 - accuracy: 0.7563\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 18s 60ms/step - loss: 0.3722 - accuracy: 0.8743\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.9245 - accuracy: 0.6625\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 14s 40ms/step - loss: 0.9064 - accuracy: 0.5987\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 12s 40ms/step - loss: 0.7587 - accuracy: 0.7013\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 12s 40ms/step - loss: 0.6116 - accuracy: 0.7668\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.8022 - accuracy: 0.6693\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 14s 40ms/step - loss: 0.8940 - accuracy: 0.6057\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 12s 40ms/step - loss: 0.7249 - accuracy: 0.7091\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 12s 40ms/step - loss: 0.5716 - accuracy: 0.7908\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.8679 - accuracy: 0.6511\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 14s 40ms/step - loss: 0.9037 - accuracy: 0.5969\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 13s 42ms/step - loss: 0.7460 - accuracy: 0.7013\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 13s 42ms/step - loss: 0.6018 - accuracy: 0.7766\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.8299 - accuracy: 0.6526\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 22s 67ms/step - loss: 0.9039 - accuracy: 0.5975\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 21s 68ms/step - loss: 0.6377 - accuracy: 0.7529\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 20s 67ms/step - loss: 0.3612 - accuracy: 0.8731\n",
      "152/152 [==============================] - 3s 15ms/step - loss: 0.9134 - accuracy: 0.6461\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 22s 68ms/step - loss: 0.8890 - accuracy: 0.6113\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 21s 68ms/step - loss: 0.6137 - accuracy: 0.7724\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 20s 67ms/step - loss: 0.3362 - accuracy: 0.8857\n",
      "152/152 [==============================] - 3s 15ms/step - loss: 0.9787 - accuracy: 0.6341\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 22s 69ms/step - loss: 0.8948 - accuracy: 0.6085\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 21s 69ms/step - loss: 0.6449 - accuracy: 0.7487\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 20s 67ms/step - loss: 0.3643 - accuracy: 0.8741\n",
      "152/152 [==============================] - 3s 15ms/step - loss: 0.9352 - accuracy: 0.6497\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 17s 50ms/step - loss: 0.9033 - accuracy: 0.5973\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 15s 51ms/step - loss: 0.7377 - accuracy: 0.7099\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 16s 52ms/step - loss: 0.5795 - accuracy: 0.7842\n",
      "152/152 [==============================] - 3s 14ms/step - loss: 0.7891 - accuracy: 0.6738\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 17s 50ms/step - loss: 0.8905 - accuracy: 0.6041\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 15s 49ms/step - loss: 0.7219 - accuracy: 0.7141\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 16s 53ms/step - loss: 0.5718 - accuracy: 0.7937\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.8694 - accuracy: 0.6445\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 17s 50ms/step - loss: 0.8889 - accuracy: 0.5976\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 15s 51ms/step - loss: 0.7176 - accuracy: 0.7117\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 15s 50ms/step - loss: 0.5634 - accuracy: 0.7916\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.8230 - accuracy: 0.6650\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 21s 64ms/step - loss: 0.9016 - accuracy: 0.6057\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 19s 64ms/step - loss: 0.6613 - accuracy: 0.7457\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 19s 63ms/step - loss: 0.3890 - accuracy: 0.8586\n",
      "152/152 [==============================] - 3s 16ms/step - loss: 0.9081 - accuracy: 0.6523\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 21s 64ms/step - loss: 0.8897 - accuracy: 0.6132\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 19s 63ms/step - loss: 0.6331 - accuracy: 0.7501\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 20s 66ms/step - loss: 0.3653 - accuracy: 0.8698\n",
      "152/152 [==============================] - 3s 14ms/step - loss: 0.9654 - accuracy: 0.6482\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 22s 64ms/step - loss: 0.8852 - accuracy: 0.6083\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 19s 63ms/step - loss: 0.6422 - accuracy: 0.7484\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 19s 63ms/step - loss: 0.3770 - accuracy: 0.8675\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.9117 - accuracy: 0.6588\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 14s 42ms/step - loss: 0.9016 - accuracy: 0.5958\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 13s 44ms/step - loss: 0.7537 - accuracy: 0.6998\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 14s 46ms/step - loss: 0.6068 - accuracy: 0.7685\n",
      "152/152 [==============================] - 3s 14ms/step - loss: 0.7853 - accuracy: 0.6602\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 15s 45ms/step - loss: 0.8918 - accuracy: 0.6051\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 13s 43ms/step - loss: 0.7205 - accuracy: 0.7097\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 13s 44ms/step - loss: 0.5835 - accuracy: 0.7773\n",
      "152/152 [==============================] - 3s 13ms/step - loss: 0.8654 - accuracy: 0.6449\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 15s 43ms/step - loss: 0.8925 - accuracy: 0.6025\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 13s 43ms/step - loss: 0.7347 - accuracy: 0.7046\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 13s 43ms/step - loss: 0.5978 - accuracy: 0.7761\n",
      "152/152 [==============================] - 4s 15ms/step - loss: 0.8234 - accuracy: 0.6683\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 25s 78ms/step - loss: 0.9027 - accuracy: 0.6037\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 23s 77ms/step - loss: 0.6281 - accuracy: 0.7515\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 23s 77ms/step - loss: 0.3564 - accuracy: 0.8689\n",
      "152/152 [==============================] - 4s 20ms/step - loss: 0.9409 - accuracy: 0.6556\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 25s 78ms/step - loss: 0.8759 - accuracy: 0.6231\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 23s 77ms/step - loss: 0.5910 - accuracy: 0.7730\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 23s 77ms/step - loss: 0.3296 - accuracy: 0.8840\n",
      "152/152 [==============================] - 3s 18ms/step - loss: 1.0133 - accuracy: 0.6267\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 25s 78ms/step - loss: 0.8815 - accuracy: 0.6085\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 24s 79ms/step - loss: 0.6269 - accuracy: 0.7578\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 24s 79ms/step - loss: 0.3484 - accuracy: 0.8743\n",
      "152/152 [==============================] - 3s 19ms/step - loss: 0.9617 - accuracy: 0.6369\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 21s 63ms/step - loss: 0.9027 - accuracy: 0.5981\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 19s 63ms/step - loss: 0.7365 - accuracy: 0.7068\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 20s 67ms/step - loss: 0.5797 - accuracy: 0.7804\n",
      "152/152 [==============================] - 3s 17ms/step - loss: 0.7956 - accuracy: 0.6602\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 16s 46ms/step - loss: 0.8824 - accuracy: 0.6076\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 13s 44ms/step - loss: 0.7026 - accuracy: 0.7217\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 13s 44ms/step - loss: 0.5521 - accuracy: 0.7972\n",
      "152/152 [==============================] - 3s 15ms/step - loss: 0.8521 - accuracy: 0.6494\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 15s 45ms/step - loss: 0.8869 - accuracy: 0.6029\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 13s 44ms/step - loss: 0.7208 - accuracy: 0.7168\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 13s 44ms/step - loss: 0.5682 - accuracy: 0.7910\n",
      "152/152 [==============================] - 3s 15ms/step - loss: 0.8268 - accuracy: 0.6609\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 21s 66ms/step - loss: 0.8948 - accuracy: 0.6115\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 20s 67ms/step - loss: 0.6089 - accuracy: 0.7618\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 20s 66ms/step - loss: 0.3309 - accuracy: 0.8770\n",
      "152/152 [==============================] - 3s 18ms/step - loss: 0.9188 - accuracy: 0.6589\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 22s 66ms/step - loss: 0.8727 - accuracy: 0.6155\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 22s 74ms/step - loss: 0.5810 - accuracy: 0.7769\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 28s 92ms/step - loss: 0.3179 - accuracy: 0.8867\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 1.0026 - accuracy: 0.6428\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 26s 83ms/step - loss: 0.8825 - accuracy: 0.6112\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 26s 85ms/step - loss: 0.6110 - accuracy: 0.7660\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 25s 84ms/step - loss: 0.3352 - accuracy: 0.8809\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.9667 - accuracy: 0.6476\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 24s 74ms/step - loss: 0.8932 - accuracy: 0.6018\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 23s 74ms/step - loss: 0.7156 - accuracy: 0.7223\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 22s 74ms/step - loss: 0.5574 - accuracy: 0.7931\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 0.7986 - accuracy: 0.6680\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 24s 75ms/step - loss: 0.8801 - accuracy: 0.6111\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 23s 76ms/step - loss: 0.6945 - accuracy: 0.7261\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 25s 82ms/step - loss: 0.5314 - accuracy: 0.8067\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 0.8956 - accuracy: 0.6573\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 24s 73ms/step - loss: 0.8801 - accuracy: 0.6089\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 23s 75ms/step - loss: 0.7031 - accuracy: 0.7224\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 22s 74ms/step - loss: 0.5444 - accuracy: 0.8018\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 0.8421 - accuracy: 0.6617\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 27s 81ms/step - loss: 0.8877 - accuracy: 0.6130\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 25s 81ms/step - loss: 0.6341 - accuracy: 0.7507\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 25s 83ms/step - loss: 0.3594 - accuracy: 0.8691\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 0.9286 - accuracy: 0.6594\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 27s 83ms/step - loss: 0.8726 - accuracy: 0.6202\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 25s 84ms/step - loss: 0.6039 - accuracy: 0.7647\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 25s 83ms/step - loss: 0.3423 - accuracy: 0.8813\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 1.0154 - accuracy: 0.6366\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 26s 82ms/step - loss: 0.8832 - accuracy: 0.6151\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 25s 81ms/step - loss: 0.6259 - accuracy: 0.7609\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 25s 81ms/step - loss: 0.3605 - accuracy: 0.8687\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 0.9410 - accuracy: 0.6555\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 23s 72ms/step - loss: 0.8928 - accuracy: 0.6039\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 22s 71ms/step - loss: 0.7398 - accuracy: 0.7062\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 21s 69ms/step - loss: 0.5921 - accuracy: 0.7716\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 0.7991 - accuracy: 0.6639\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 22s 69ms/step - loss: 0.8743 - accuracy: 0.6171\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 21s 70ms/step - loss: 0.7017 - accuracy: 0.7199\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 22s 71ms/step - loss: 0.5614 - accuracy: 0.7889\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.8922 - accuracy: 0.6569\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 23s 70ms/step - loss: 0.8805 - accuracy: 0.6081\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 22s 73ms/step - loss: 0.7202 - accuracy: 0.7185\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 22s 72ms/step - loss: 0.5798 - accuracy: 0.7797\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.8475 - accuracy: 0.6414\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 33s 105ms/step - loss: 0.8880 - accuracy: 0.6119\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 33s 107ms/step - loss: 0.6033 - accuracy: 0.7616\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 32s 105ms/step - loss: 0.3426 - accuracy: 0.8755\n",
      "152/152 [==============================] - 5s 33ms/step - loss: 0.9561 - accuracy: 0.6362\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 35s 111ms/step - loss: 0.8659 - accuracy: 0.6241\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 33s 108ms/step - loss: 0.5831 - accuracy: 0.7775\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 34s 113ms/step - loss: 0.3124 - accuracy: 0.8859\n",
      "152/152 [==============================] - 5s 33ms/step - loss: 1.0574 - accuracy: 0.6370\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 33s 104ms/step - loss: 0.8708 - accuracy: 0.6172\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 32s 106ms/step - loss: 0.5857 - accuracy: 0.7702\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 32s 107ms/step - loss: 0.3297 - accuracy: 0.8836\n",
      "152/152 [==============================] - 6s 33ms/step - loss: 0.9820 - accuracy: 0.6439\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 31s 97ms/step - loss: 0.8903 - accuracy: 0.6103\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 29s 96ms/step - loss: 0.7065 - accuracy: 0.7190\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 29s 96ms/step - loss: 0.5458 - accuracy: 0.7941\n",
      "152/152 [==============================] - 5s 33ms/step - loss: 0.7980 - accuracy: 0.6714\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 30s 95ms/step - loss: 0.8654 - accuracy: 0.6235\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 29s 96ms/step - loss: 0.6770 - accuracy: 0.7341\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 30s 98ms/step - loss: 0.5145 - accuracy: 0.8162\n",
      "152/152 [==============================] - 6s 33ms/step - loss: 0.8863 - accuracy: 0.6499\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 31s 97ms/step - loss: 0.8783 - accuracy: 0.6036\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 29s 97ms/step - loss: 0.6953 - accuracy: 0.7253\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 29s 95ms/step - loss: 0.5345 - accuracy: 0.8024\n",
      "152/152 [==============================] - 6s 34ms/step - loss: 0.8473 - accuracy: 0.6406\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 47s 150ms/step - loss: 0.8841 - accuracy: 0.6099\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 46s 153ms/step - loss: 0.5986 - accuracy: 0.7693\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 46s 152ms/step - loss: 0.3229 - accuracy: 0.8789\n",
      "152/152 [==============================] - 7s 42ms/step - loss: 0.9802 - accuracy: 0.6614\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 47s 152ms/step - loss: 0.8677 - accuracy: 0.6248\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 46s 153ms/step - loss: 0.5692 - accuracy: 0.7840\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 47s 155ms/step - loss: 0.2952 - accuracy: 0.8882\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 1.0165 - accuracy: 0.6313\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 49s 158ms/step - loss: 0.8733 - accuracy: 0.6153\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 47s 154ms/step - loss: 0.5747 - accuracy: 0.7819\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 47s 153ms/step - loss: 0.3123 - accuracy: 0.8809\n",
      "152/152 [==============================] - 7s 41ms/step - loss: 1.0164 - accuracy: 0.6514\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 44s 141ms/step - loss: 0.8821 - accuracy: 0.6029\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 41s 134ms/step - loss: 0.6826 - accuracy: 0.7277\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 41s 135ms/step - loss: 0.5154 - accuracy: 0.8059\n",
      "152/152 [==============================] - 6s 38ms/step - loss: 0.8724 - accuracy: 0.6242\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 42s 135ms/step - loss: 0.8779 - accuracy: 0.6064\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 40s 133ms/step - loss: 0.6696 - accuracy: 0.7362\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 42s 138ms/step - loss: 0.4975 - accuracy: 0.8162\n",
      "152/152 [==============================] - 6s 38ms/step - loss: 0.9065 - accuracy: 0.6536\n",
      "Epoch 1/3\n",
      "303/303 [==============================] - 41s 132ms/step - loss: 0.8741 - accuracy: 0.6114\n",
      "Epoch 2/3\n",
      "303/303 [==============================] - 40s 131ms/step - loss: 0.6785 - accuracy: 0.7356\n",
      "Epoch 3/3\n",
      "303/303 [==============================] - 43s 142ms/step - loss: 0.5052 - accuracy: 0.8208\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.8432 - accuracy: 0.6687\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 15s 45ms/step - loss: 0.9155 - accuracy: 0.5995\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 14s 46ms/step - loss: 0.6752 - accuracy: 0.7381\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 14s 45ms/step - loss: 0.4041 - accuracy: 0.8615\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 14s 46ms/step - loss: 0.2551 - accuracy: 0.9074\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 14s 46ms/step - loss: 0.1880 - accuracy: 0.9289\n",
      "152/152 [==============================] - 2s 10ms/step - loss: 1.0444 - accuracy: 0.6515\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 15s 45ms/step - loss: 0.8971 - accuracy: 0.6142\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 14s 46ms/step - loss: 0.6470 - accuracy: 0.7474\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 14s 47ms/step - loss: 0.3843 - accuracy: 0.8652\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 14s 46ms/step - loss: 0.2366 - accuracy: 0.9163\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 14s 45ms/step - loss: 0.1663 - accuracy: 0.9444\n",
      "152/152 [==============================] - 2s 9ms/step - loss: 1.1483 - accuracy: 0.6403\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 15s 45ms/step - loss: 0.9014 - accuracy: 0.6064\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 14s 45ms/step - loss: 0.6644 - accuracy: 0.7371\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 14s 45ms/step - loss: 0.3935 - accuracy: 0.8615\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 14s 45ms/step - loss: 0.2442 - accuracy: 0.9109\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 14s 46ms/step - loss: 0.1726 - accuracy: 0.9357\n",
      "152/152 [==============================] - 2s 9ms/step - loss: 1.0926 - accuracy: 0.6505\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 11s 32ms/step - loss: 0.9005 - accuracy: 0.5925\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 10s 32ms/step - loss: 0.7728 - accuracy: 0.6913\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 10s 32ms/step - loss: 0.6474 - accuracy: 0.7519\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 10s 33ms/step - loss: 0.5310 - accuracy: 0.8086\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 10s 33ms/step - loss: 0.4319 - accuracy: 0.8466\n",
      "152/152 [==============================] - 2s 10ms/step - loss: 0.8386 - accuracy: 0.6660\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 11s 32ms/step - loss: 0.8955 - accuracy: 0.6049\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 10s 31ms/step - loss: 0.7392 - accuracy: 0.7000\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 10s 32ms/step - loss: 0.6061 - accuracy: 0.7730\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 10s 32ms/step - loss: 0.4957 - accuracy: 0.8239\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 10s 32ms/step - loss: 0.4020 - accuracy: 0.8600\n",
      "152/152 [==============================] - 2s 10ms/step - loss: 0.9082 - accuracy: 0.6585\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 11s 33ms/step - loss: 0.8997 - accuracy: 0.6048\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 10s 32ms/step - loss: 0.7533 - accuracy: 0.6943\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 10s 33ms/step - loss: 0.6222 - accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 10s 33ms/step - loss: 0.5084 - accuracy: 0.8187\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 10s 33ms/step - loss: 0.4149 - accuracy: 0.8551\n",
      "152/152 [==============================] - 2s 9ms/step - loss: 0.9019 - accuracy: 0.6361\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 19s 58ms/step - loss: 0.9058 - accuracy: 0.5995\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 18s 59ms/step - loss: 0.6527 - accuracy: 0.7501\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 18s 60ms/step - loss: 0.3880 - accuracy: 0.8636\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 18s 58ms/step - loss: 0.2388 - accuracy: 0.9125\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 18s 59ms/step - loss: 0.1677 - accuracy: 0.9369\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 1.0467 - accuracy: 0.6499\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 19s 55ms/step - loss: 0.9017 - accuracy: 0.6066\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 17s 56ms/step - loss: 0.6234 - accuracy: 0.7594\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 17s 54ms/step - loss: 0.3535 - accuracy: 0.8747\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 17s 55ms/step - loss: 0.2133 - accuracy: 0.9245\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 17s 56ms/step - loss: 0.1458 - accuracy: 0.9506\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 1.1670 - accuracy: 0.6197\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 18s 54ms/step - loss: 0.8973 - accuracy: 0.6108\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 17s 57ms/step - loss: 0.6320 - accuracy: 0.7544\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 17s 57ms/step - loss: 0.3635 - accuracy: 0.8677\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 17s 56ms/step - loss: 0.2187 - accuracy: 0.9212\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 17s 56ms/step - loss: 0.1481 - accuracy: 0.9456\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 1.0898 - accuracy: 0.6563\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 14s 40ms/step - loss: 0.8999 - accuracy: 0.6008\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 12s 41ms/step - loss: 0.7467 - accuracy: 0.7066\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 12s 40ms/step - loss: 0.6072 - accuracy: 0.7678\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 12s 40ms/step - loss: 0.4839 - accuracy: 0.8257\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 12s 39ms/step - loss: 0.3816 - accuracy: 0.8687\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.8890 - accuracy: 0.6701\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 14s 40ms/step - loss: 0.8892 - accuracy: 0.6053\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 13s 42ms/step - loss: 0.7224 - accuracy: 0.7087\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 12s 39ms/step - loss: 0.5702 - accuracy: 0.7912\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 12s 39ms/step - loss: 0.4487 - accuracy: 0.8410\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 12s 39ms/step - loss: 0.3502 - accuracy: 0.8809\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.9582 - accuracy: 0.6222\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 13s 38ms/step - loss: 0.8870 - accuracy: 0.5992\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 12s 40ms/step - loss: 0.7396 - accuracy: 0.7063\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 12s 39ms/step - loss: 0.6012 - accuracy: 0.7737\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 12s 41ms/step - loss: 0.4835 - accuracy: 0.8268\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 12s 40ms/step - loss: 0.3777 - accuracy: 0.8690\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.9038 - accuracy: 0.6572\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 21s 65ms/step - loss: 0.9049 - accuracy: 0.5985\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 20s 66ms/step - loss: 0.6441 - accuracy: 0.7480\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 20s 67ms/step - loss: 0.3650 - accuracy: 0.8741\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 20s 67ms/step - loss: 0.2179 - accuracy: 0.9221\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 20s 66ms/step - loss: 0.1440 - accuracy: 0.9473\n",
      "152/152 [==============================] - 3s 15ms/step - loss: 1.0745 - accuracy: 0.6482\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 21s 66ms/step - loss: 0.8962 - accuracy: 0.6037\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 20s 66ms/step - loss: 0.6126 - accuracy: 0.7618\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 20s 66ms/step - loss: 0.3340 - accuracy: 0.8844\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 21s 68ms/step - loss: 0.2007 - accuracy: 0.9283\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 20s 68ms/step - loss: 0.1306 - accuracy: 0.9556\n",
      "152/152 [==============================] - 3s 15ms/step - loss: 1.1727 - accuracy: 0.6403\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 22s 67ms/step - loss: 0.8921 - accuracy: 0.6015\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 21s 68ms/step - loss: 0.6349 - accuracy: 0.7542\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 20s 67ms/step - loss: 0.3530 - accuracy: 0.8772\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 20s 66ms/step - loss: 0.2145 - accuracy: 0.9194\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 19s 64ms/step - loss: 0.1387 - accuracy: 0.9502\n",
      "152/152 [==============================] - 3s 14ms/step - loss: 1.1576 - accuracy: 0.6365\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 16s 48ms/step - loss: 0.9052 - accuracy: 0.6043\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 15s 49ms/step - loss: 0.7487 - accuracy: 0.7058\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 15s 49ms/step - loss: 0.5964 - accuracy: 0.7726\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 15s 49ms/step - loss: 0.4650 - accuracy: 0.8356\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 14s 48ms/step - loss: 0.3578 - accuracy: 0.8743\n",
      "152/152 [==============================] - 3s 15ms/step - loss: 0.8855 - accuracy: 0.6565\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 16s 47ms/step - loss: 0.8891 - accuracy: 0.6037\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 15s 49ms/step - loss: 0.7116 - accuracy: 0.7234\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 15s 48ms/step - loss: 0.5568 - accuracy: 0.7951\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 15s 49ms/step - loss: 0.4297 - accuracy: 0.8542\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 15s 49ms/step - loss: 0.3293 - accuracy: 0.8929\n",
      "152/152 [==============================] - 3s 16ms/step - loss: 0.9599 - accuracy: 0.6437\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 17s 49ms/step - loss: 0.8912 - accuracy: 0.6054\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 14s 48ms/step - loss: 0.7304 - accuracy: 0.7036\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 14s 48ms/step - loss: 0.5779 - accuracy: 0.7865\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 14s 47ms/step - loss: 0.4501 - accuracy: 0.8468\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 14s 46ms/step - loss: 0.3507 - accuracy: 0.8834\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.9183 - accuracy: 0.6435\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 20s 61ms/step - loss: 0.9033 - accuracy: 0.6008\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 18s 60ms/step - loss: 0.6462 - accuracy: 0.7436\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 18s 60ms/step - loss: 0.3807 - accuracy: 0.8627\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 19s 61ms/step - loss: 0.2363 - accuracy: 0.9119\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 19s 61ms/step - loss: 0.1651 - accuracy: 0.9349\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 1.1479 - accuracy: 0.6519\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 21s 64ms/step - loss: 0.8880 - accuracy: 0.6064\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 19s 64ms/step - loss: 0.6296 - accuracy: 0.7507\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 19s 63ms/step - loss: 0.3661 - accuracy: 0.8687\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 20s 65ms/step - loss: 0.2228 - accuracy: 0.9190\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 20s 65ms/step - loss: 0.1581 - accuracy: 0.9434\n",
      "152/152 [==============================] - 3s 14ms/step - loss: 1.1618 - accuracy: 0.6271\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 21s 65ms/step - loss: 0.8863 - accuracy: 0.6124\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 20s 66ms/step - loss: 0.6398 - accuracy: 0.7501\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 20s 65ms/step - loss: 0.3765 - accuracy: 0.8683\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 20s 64ms/step - loss: 0.2318 - accuracy: 0.9095\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 20s 65ms/step - loss: 0.1639 - accuracy: 0.9378\n",
      "152/152 [==============================] - 3s 15ms/step - loss: 1.1622 - accuracy: 0.6485\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 15s 44ms/step - loss: 0.9007 - accuracy: 0.6029\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 14s 47ms/step - loss: 0.7524 - accuracy: 0.7006\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 13s 44ms/step - loss: 0.6152 - accuracy: 0.7635\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 14s 45ms/step - loss: 0.4958 - accuracy: 0.8249\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 14s 45ms/step - loss: 0.4013 - accuracy: 0.8545\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.8760 - accuracy: 0.6536\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 15s 43ms/step - loss: 0.8774 - accuracy: 0.6130\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 12s 39ms/step - loss: 0.7235 - accuracy: 0.7097\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 12s 41ms/step - loss: 0.5869 - accuracy: 0.7817\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 12s 40ms/step - loss: 0.4734 - accuracy: 0.8323\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 12s 39ms/step - loss: 0.3824 - accuracy: 0.8662\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.9297 - accuracy: 0.6478\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 14s 40ms/step - loss: 0.8912 - accuracy: 0.6038\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 12s 40ms/step - loss: 0.7444 - accuracy: 0.6959\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 13s 42ms/step - loss: 0.6096 - accuracy: 0.7648\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 13s 42ms/step - loss: 0.4959 - accuracy: 0.8181\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 13s 42ms/step - loss: 0.4047 - accuracy: 0.8563\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.9001 - accuracy: 0.6563\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 22s 70ms/step - loss: 0.8958 - accuracy: 0.6051\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 22s 72ms/step - loss: 0.6295 - accuracy: 0.7577\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 21s 71ms/step - loss: 0.3548 - accuracy: 0.8716\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 21s 71ms/step - loss: 0.2162 - accuracy: 0.9214\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 22s 71ms/step - loss: 0.1435 - accuracy: 0.9479\n",
      "152/152 [==============================] - 3s 16ms/step - loss: 1.1539 - accuracy: 0.6552\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 22s 70ms/step - loss: 0.8777 - accuracy: 0.6126\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 21s 69ms/step - loss: 0.5949 - accuracy: 0.7691\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 21s 69ms/step - loss: 0.3352 - accuracy: 0.8807\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 21s 69ms/step - loss: 0.2019 - accuracy: 0.9258\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 21s 71ms/step - loss: 0.1287 - accuracy: 0.9541\n",
      "152/152 [==============================] - 3s 17ms/step - loss: 1.2258 - accuracy: 0.6259\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 22s 69ms/step - loss: 0.8910 - accuracy: 0.6081\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 21s 70ms/step - loss: 0.6195 - accuracy: 0.7660\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 22s 71ms/step - loss: 0.3509 - accuracy: 0.8752\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 21s 70ms/step - loss: 0.2137 - accuracy: 0.9194\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 21s 71ms/step - loss: 0.1429 - accuracy: 0.9477\n",
      "152/152 [==============================] - 3s 17ms/step - loss: 1.2200 - accuracy: 0.6435\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 19s 58ms/step - loss: 0.9025 - accuracy: 0.6012\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 17s 56ms/step - loss: 0.7330 - accuracy: 0.7039\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 16s 54ms/step - loss: 0.5811 - accuracy: 0.7796\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 17s 56ms/step - loss: 0.4535 - accuracy: 0.8394\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 17s 55ms/step - loss: 0.3542 - accuracy: 0.8724\n",
      "152/152 [==============================] - 3s 17ms/step - loss: 0.8884 - accuracy: 0.6639\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 19s 57ms/step - loss: 0.8758 - accuracy: 0.6134\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 17s 57ms/step - loss: 0.6998 - accuracy: 0.7209\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 18s 58ms/step - loss: 0.5514 - accuracy: 0.7945\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 17s 58ms/step - loss: 0.4233 - accuracy: 0.8462\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 18s 61ms/step - loss: 0.3245 - accuracy: 0.8859\n",
      "152/152 [==============================] - 3s 17ms/step - loss: 0.9567 - accuracy: 0.6428\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 18s 56ms/step - loss: 0.8834 - accuracy: 0.6044\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 17s 56ms/step - loss: 0.7195 - accuracy: 0.7104\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 17s 57ms/step - loss: 0.5706 - accuracy: 0.7877\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 17s 58ms/step - loss: 0.4436 - accuracy: 0.8377\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 18s 59ms/step - loss: 0.3423 - accuracy: 0.8828\n",
      "152/152 [==============================] - 3s 17ms/step - loss: 0.9446 - accuracy: 0.6522\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 26s 81ms/step - loss: 0.8900 - accuracy: 0.6029\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 24s 80ms/step - loss: 0.6094 - accuracy: 0.7604\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 24s 81ms/step - loss: 0.3523 - accuracy: 0.8751\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 24s 80ms/step - loss: 0.2154 - accuracy: 0.9194\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 24s 80ms/step - loss: 0.1429 - accuracy: 0.9442\n",
      "152/152 [==============================] - 4s 21ms/step - loss: 1.1325 - accuracy: 0.6482\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 26s 81ms/step - loss: 0.8766 - accuracy: 0.6140\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 25s 82ms/step - loss: 0.5890 - accuracy: 0.7726\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 25s 81ms/step - loss: 0.3203 - accuracy: 0.8873\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 26s 84ms/step - loss: 0.1937 - accuracy: 0.9361\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 25s 83ms/step - loss: 0.1215 - accuracy: 0.9603\n",
      "152/152 [==============================] - 5s 23ms/step - loss: 1.3073 - accuracy: 0.6267\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 26s 82ms/step - loss: 0.8784 - accuracy: 0.6133\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 24s 80ms/step - loss: 0.6019 - accuracy: 0.7683\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 25s 82ms/step - loss: 0.3320 - accuracy: 0.8768\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 26s 86ms/step - loss: 0.1988 - accuracy: 0.9285\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 25s 82ms/step - loss: 0.1326 - accuracy: 0.9502\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 1.2209 - accuracy: 0.6530\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 23s 70ms/step - loss: 0.8926 - accuracy: 0.6053\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 22s 71ms/step - loss: 0.7110 - accuracy: 0.7161\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 22s 73ms/step - loss: 0.5521 - accuracy: 0.7953\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 21s 70ms/step - loss: 0.4189 - accuracy: 0.8466\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 22s 71ms/step - loss: 0.3091 - accuracy: 0.8921\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 0.9296 - accuracy: 0.6652\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 23s 73ms/step - loss: 0.8802 - accuracy: 0.6070\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 22s 72ms/step - loss: 0.6901 - accuracy: 0.7298\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 22s 74ms/step - loss: 0.5262 - accuracy: 0.8071\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 22s 72ms/step - loss: 0.3988 - accuracy: 0.8633\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 22s 71ms/step - loss: 0.2931 - accuracy: 0.8958\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 0.9865 - accuracy: 0.6527\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 23s 72ms/step - loss: 0.8806 - accuracy: 0.6040\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 22s 71ms/step - loss: 0.7087 - accuracy: 0.7210\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 22s 71ms/step - loss: 0.5479 - accuracy: 0.7941\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 22s 73ms/step - loss: 0.4182 - accuracy: 0.8543\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 22s 72ms/step - loss: 0.3138 - accuracy: 0.8944\n",
      "152/152 [==============================] - 4s 21ms/step - loss: 0.9513 - accuracy: 0.6584\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 25s 80ms/step - loss: 0.8926 - accuracy: 0.6115\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 24s 79ms/step - loss: 0.6380 - accuracy: 0.7438\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 26s 86ms/step - loss: 0.3718 - accuracy: 0.8706\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 26s 86ms/step - loss: 0.2239 - accuracy: 0.9167\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 26s 87ms/step - loss: 0.1660 - accuracy: 0.9363\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 1.1576 - accuracy: 0.6544\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 27s 85ms/step - loss: 0.8694 - accuracy: 0.6241\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 26s 86ms/step - loss: 0.5986 - accuracy: 0.7635\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 26s 85ms/step - loss: 0.3395 - accuracy: 0.8803\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 26s 87ms/step - loss: 0.2073 - accuracy: 0.9264\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 26s 86ms/step - loss: 0.1433 - accuracy: 0.9485\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 1.3178 - accuracy: 0.6304\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 27s 85ms/step - loss: 0.8813 - accuracy: 0.6075\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 26s 86ms/step - loss: 0.6248 - accuracy: 0.7549\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 26s 86ms/step - loss: 0.3612 - accuracy: 0.8710\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 26s 86ms/step - loss: 0.2261 - accuracy: 0.9194\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 26s 86ms/step - loss: 0.1613 - accuracy: 0.9374\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 1.1908 - accuracy: 0.6505\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 25s 77ms/step - loss: 0.8949 - accuracy: 0.6022\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 22s 74ms/step - loss: 0.7322 - accuracy: 0.7075\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 22s 74ms/step - loss: 0.5819 - accuracy: 0.7809\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 22s 74ms/step - loss: 0.4560 - accuracy: 0.8356\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 22s 73ms/step - loss: 0.3598 - accuracy: 0.8729\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.9131 - accuracy: 0.6581\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 24s 74ms/step - loss: 0.8714 - accuracy: 0.6192\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 22s 73ms/step - loss: 0.7023 - accuracy: 0.7182\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 22s 74ms/step - loss: 0.5660 - accuracy: 0.7883\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 23s 75ms/step - loss: 0.4528 - accuracy: 0.8346\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 23s 75ms/step - loss: 0.3583 - accuracy: 0.8751\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.9609 - accuracy: 0.6213\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 24s 74ms/step - loss: 0.8807 - accuracy: 0.6021\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 23s 76ms/step - loss: 0.7195 - accuracy: 0.7065\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 23s 75ms/step - loss: 0.5767 - accuracy: 0.7879\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 23s 75ms/step - loss: 0.4559 - accuracy: 0.8369\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 23s 76ms/step - loss: 0.3681 - accuracy: 0.8708\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.9980 - accuracy: 0.6162\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 35s 109ms/step - loss: 0.8812 - accuracy: 0.6157\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 32s 107ms/step - loss: 0.6127 - accuracy: 0.7651\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 33s 108ms/step - loss: 0.3414 - accuracy: 0.8780\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 34s 111ms/step - loss: 0.2118 - accuracy: 0.9250\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 33s 110ms/step - loss: 0.1430 - accuracy: 0.9436\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 1.2279 - accuracy: 0.6234\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 35s 110ms/step - loss: 0.8638 - accuracy: 0.6182\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 32s 105ms/step - loss: 0.5819 - accuracy: 0.7757\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 31s 101ms/step - loss: 0.3155 - accuracy: 0.8863\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 31s 101ms/step - loss: 0.1921 - accuracy: 0.9357\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 31s 101ms/step - loss: 0.1198 - accuracy: 0.9562\n",
      "152/152 [==============================] - 5s 31ms/step - loss: 1.2918 - accuracy: 0.6271\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 33s 102ms/step - loss: 0.8806 - accuracy: 0.6104\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 31s 101ms/step - loss: 0.5987 - accuracy: 0.7635\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 30s 100ms/step - loss: 0.3320 - accuracy: 0.8797\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 31s 101ms/step - loss: 0.2037 - accuracy: 0.9241\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 31s 101ms/step - loss: 0.1349 - accuracy: 0.9508\n",
      "152/152 [==============================] - 5s 32ms/step - loss: 1.2443 - accuracy: 0.6526\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 30s 93ms/step - loss: 0.8879 - accuracy: 0.6093\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 28s 92ms/step - loss: 0.7102 - accuracy: 0.7137\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 31s 102ms/step - loss: 0.5518 - accuracy: 0.7900\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 30s 100ms/step - loss: 0.4213 - accuracy: 0.8474\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 31s 103ms/step - loss: 0.3267 - accuracy: 0.8830\n",
      "152/152 [==============================] - 5s 33ms/step - loss: 0.9185 - accuracy: 0.6709\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 32s 101ms/step - loss: 0.8728 - accuracy: 0.6142\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 33s 109ms/step - loss: 0.6820 - accuracy: 0.7335\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 31s 101ms/step - loss: 0.5233 - accuracy: 0.8096\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 31s 104ms/step - loss: 0.4004 - accuracy: 0.8623\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 31s 102ms/step - loss: 0.2997 - accuracy: 0.8941\n",
      "152/152 [==============================] - 6s 36ms/step - loss: 1.1485 - accuracy: 0.5713\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 33s 102ms/step - loss: 0.8768 - accuracy: 0.6071\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 31s 103ms/step - loss: 0.6956 - accuracy: 0.7239\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 31s 102ms/step - loss: 0.5349 - accuracy: 0.7991\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 27s 90ms/step - loss: 0.4108 - accuracy: 0.8532\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 27s 89ms/step - loss: 0.3125 - accuracy: 0.8888\n",
      "152/152 [==============================] - 5s 30ms/step - loss: 0.9576 - accuracy: 0.6394\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 44s 142ms/step - loss: 0.8834 - accuracy: 0.6086\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 43s 141ms/step - loss: 0.5911 - accuracy: 0.7709\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 43s 141ms/step - loss: 0.3230 - accuracy: 0.8791\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 43s 143ms/step - loss: 0.1938 - accuracy: 0.9293\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 44s 146ms/step - loss: 0.1238 - accuracy: 0.9541\n",
      "152/152 [==============================] - 6s 39ms/step - loss: 1.1706 - accuracy: 0.6499\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 46s 148ms/step - loss: 0.8626 - accuracy: 0.6229\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 47s 156ms/step - loss: 0.5540 - accuracy: 0.7943\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 50s 163ms/step - loss: 0.2916 - accuracy: 0.8935\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 49s 161ms/step - loss: 0.1669 - accuracy: 0.9425\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 51s 167ms/step - loss: 0.1111 - accuracy: 0.9622\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 1.3286 - accuracy: 0.6350\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 51s 165ms/step - loss: 0.8755 - accuracy: 0.6147\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 49s 161ms/step - loss: 0.5844 - accuracy: 0.7788\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 50s 163ms/step - loss: 0.3212 - accuracy: 0.8807\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 50s 164ms/step - loss: 0.1895 - accuracy: 0.9264\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 50s 166ms/step - loss: 0.1285 - accuracy: 0.9541\n",
      "152/152 [==============================] - 8s 47ms/step - loss: 1.2408 - accuracy: 0.6414\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 50s 155ms/step - loss: 0.8850 - accuracy: 0.6107\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 47s 155ms/step - loss: 0.6969 - accuracy: 0.7203\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 46s 153ms/step - loss: 0.5174 - accuracy: 0.8104\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 47s 154ms/step - loss: 0.3802 - accuracy: 0.8623\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 46s 152ms/step - loss: 0.2773 - accuracy: 0.8997\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.9373 - accuracy: 0.6623\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 47s 147ms/step - loss: 0.8660 - accuracy: 0.6184\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 45s 148ms/step - loss: 0.6615 - accuracy: 0.7441\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 45s 149ms/step - loss: 0.4972 - accuracy: 0.8179\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 45s 149ms/step - loss: 0.3619 - accuracy: 0.8718\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 45s 149ms/step - loss: 0.2639 - accuracy: 0.9059\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 1.1289 - accuracy: 0.5949\n",
      "Epoch 1/5\n",
      "303/303 [==============================] - 47s 149ms/step - loss: 0.8797 - accuracy: 0.6000\n",
      "Epoch 2/5\n",
      "303/303 [==============================] - 45s 147ms/step - loss: 0.6746 - accuracy: 0.7369\n",
      "Epoch 3/5\n",
      "303/303 [==============================] - 45s 148ms/step - loss: 0.5072 - accuracy: 0.8160\n",
      "Epoch 4/5\n",
      "303/303 [==============================] - 45s 148ms/step - loss: 0.3717 - accuracy: 0.8706\n",
      "Epoch 5/5\n",
      "303/303 [==============================] - 44s 144ms/step - loss: 0.2667 - accuracy: 0.9064\n",
      "152/152 [==============================] - 7s 42ms/step - loss: 1.0173 - accuracy: 0.6299\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 12s 68ms/step - loss: 0.9572 - accuracy: 0.5824\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.7508 - accuracy: 0.6984\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.4999 - accuracy: 0.8162\n",
      "76/76 [==============================] - 2s 15ms/step - loss: 0.8254 - accuracy: 0.6561\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 12s 67ms/step - loss: 0.9448 - accuracy: 0.5731\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 10s 68ms/step - loss: 0.7170 - accuracy: 0.7130\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 10s 67ms/step - loss: 0.4757 - accuracy: 0.8290\n",
      "76/76 [==============================] - 2s 16ms/step - loss: 0.8609 - accuracy: 0.6606\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 12s 66ms/step - loss: 0.9414 - accuracy: 0.5916\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 10s 67ms/step - loss: 0.7386 - accuracy: 0.7127\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 10s 67ms/step - loss: 0.5044 - accuracy: 0.8216\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 0.8435 - accuracy: 0.6625\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 9s 48ms/step - loss: 0.9267 - accuracy: 0.5909\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.7901 - accuracy: 0.6765\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.6608 - accuracy: 0.7534\n",
      "76/76 [==============================] - 2s 15ms/step - loss: 0.7962 - accuracy: 0.6602\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 9s 49ms/step - loss: 0.9055 - accuracy: 0.5975\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.7652 - accuracy: 0.6847\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.6328 - accuracy: 0.7567\n",
      "76/76 [==============================] - 2s 16ms/step - loss: 0.8394 - accuracy: 0.6428\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 9s 51ms/step - loss: 0.9240 - accuracy: 0.5918\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.7770 - accuracy: 0.6784\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.6493 - accuracy: 0.7513\n",
      "76/76 [==============================] - 2s 15ms/step - loss: 0.8025 - accuracy: 0.6671\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 14s 83ms/step - loss: 0.9346 - accuracy: 0.5923\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 13s 86ms/step - loss: 0.7301 - accuracy: 0.7157\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 13s 83ms/step - loss: 0.4707 - accuracy: 0.8296\n",
      "76/76 [==============================] - 2s 19ms/step - loss: 0.8393 - accuracy: 0.6594\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 14s 83ms/step - loss: 0.9170 - accuracy: 0.6006\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 13s 86ms/step - loss: 0.6887 - accuracy: 0.7306\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 13s 85ms/step - loss: 0.4289 - accuracy: 0.8470\n",
      "76/76 [==============================] - 2s 20ms/step - loss: 0.9001 - accuracy: 0.6317\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 14s 84ms/step - loss: 0.9245 - accuracy: 0.5953\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 13s 84ms/step - loss: 0.6985 - accuracy: 0.7298\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 13s 83ms/step - loss: 0.4350 - accuracy: 0.8499\n",
      "76/76 [==============================] - 2s 19ms/step - loss: 0.8749 - accuracy: 0.6572\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 12s 72ms/step - loss: 0.9269 - accuracy: 0.5884\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 11s 73ms/step - loss: 0.7636 - accuracy: 0.7027\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.6213 - accuracy: 0.7676\n",
      "76/76 [==============================] - 2s 21ms/step - loss: 0.8087 - accuracy: 0.6370\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 12s 71ms/step - loss: 0.9117 - accuracy: 0.5954\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.7590 - accuracy: 0.6903\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 11s 73ms/step - loss: 0.6086 - accuracy: 0.7720\n",
      "76/76 [==============================] - 2s 20ms/step - loss: 0.8387 - accuracy: 0.6482\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 13s 73ms/step - loss: 0.9168 - accuracy: 0.5878\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.7647 - accuracy: 0.6844\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.6259 - accuracy: 0.7662\n",
      "76/76 [==============================] - 2s 21ms/step - loss: 0.8233 - accuracy: 0.6567\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 16s 97ms/step - loss: 0.9252 - accuracy: 0.5909\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.7046 - accuracy: 0.7302\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 15s 96ms/step - loss: 0.4369 - accuracy: 0.8472\n",
      "76/76 [==============================] - 2s 25ms/step - loss: 0.8728 - accuracy: 0.6490\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 16s 97ms/step - loss: 0.9177 - accuracy: 0.6012\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 15s 100ms/step - loss: 0.6823 - accuracy: 0.7387\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 15s 99ms/step - loss: 0.4106 - accuracy: 0.8615\n",
      "76/76 [==============================] - 2s 25ms/step - loss: 0.9251 - accuracy: 0.6432\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 16s 100ms/step - loss: 0.9227 - accuracy: 0.5843\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 15s 101ms/step - loss: 0.7075 - accuracy: 0.7158\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.4397 - accuracy: 0.8431\n",
      "76/76 [==============================] - 2s 25ms/step - loss: 0.8939 - accuracy: 0.6406\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 15s 86ms/step - loss: 0.9130 - accuracy: 0.5907\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 13s 84ms/step - loss: 0.7706 - accuracy: 0.6965\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 13s 86ms/step - loss: 0.6166 - accuracy: 0.7711\n",
      "76/76 [==============================] - 2s 25ms/step - loss: 0.7904 - accuracy: 0.6709\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 15s 86ms/step - loss: 0.9144 - accuracy: 0.5960\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 13s 84ms/step - loss: 0.7445 - accuracy: 0.7002\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 13s 85ms/step - loss: 0.5777 - accuracy: 0.7933\n",
      "76/76 [==============================] - 2s 26ms/step - loss: 0.8452 - accuracy: 0.6569\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 14s 86ms/step - loss: 0.9034 - accuracy: 0.5947\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 14s 89ms/step - loss: 0.7466 - accuracy: 0.7024\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 13s 87ms/step - loss: 0.5936 - accuracy: 0.7780\n",
      "76/76 [==============================] - 2s 24ms/step - loss: 0.8268 - accuracy: 0.6510\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 15s 89ms/step - loss: 0.9301 - accuracy: 0.5896\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 13s 88ms/step - loss: 0.7265 - accuracy: 0.7157\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 14s 90ms/step - loss: 0.4640 - accuracy: 0.8313\n",
      "76/76 [==============================] - 2s 25ms/step - loss: 0.8506 - accuracy: 0.6660\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 15s 88ms/step - loss: 0.9174 - accuracy: 0.6080\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 14s 89ms/step - loss: 0.6834 - accuracy: 0.7337\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 13s 88ms/step - loss: 0.4396 - accuracy: 0.8416\n",
      "76/76 [==============================] - 2s 24ms/step - loss: 0.9045 - accuracy: 0.6536\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 15s 88ms/step - loss: 0.9308 - accuracy: 0.5945\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.7126 - accuracy: 0.7220\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 14s 90ms/step - loss: 0.4634 - accuracy: 0.8336\n",
      "76/76 [==============================] - 2s 25ms/step - loss: 0.8692 - accuracy: 0.6646\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 14s 80ms/step - loss: 0.9148 - accuracy: 0.5969\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 12s 79ms/step - loss: 0.7650 - accuracy: 0.6971\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 12s 80ms/step - loss: 0.6319 - accuracy: 0.7631\n",
      "76/76 [==============================] - 2s 24ms/step - loss: 0.7932 - accuracy: 0.6598\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 14s 78ms/step - loss: 0.8954 - accuracy: 0.6020\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 12s 79ms/step - loss: 0.7457 - accuracy: 0.6990\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 12s 77ms/step - loss: 0.6111 - accuracy: 0.7676\n",
      "76/76 [==============================] - 2s 23ms/step - loss: 0.8472 - accuracy: 0.6478\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 13s 77ms/step - loss: 0.9066 - accuracy: 0.5967\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 12s 77ms/step - loss: 0.7650 - accuracy: 0.6807\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 12s 78ms/step - loss: 0.6212 - accuracy: 0.7650\n",
      "76/76 [==============================] - 2s 26ms/step - loss: 0.8370 - accuracy: 0.6274\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 19s 116ms/step - loss: 0.9319 - accuracy: 0.5867\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 17s 115ms/step - loss: 0.6960 - accuracy: 0.7279\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.4286 - accuracy: 0.8489\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.9234 - accuracy: 0.6329\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 19s 115ms/step - loss: 0.9046 - accuracy: 0.5956\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.6659 - accuracy: 0.7511\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.4060 - accuracy: 0.8576\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.9372 - accuracy: 0.6523\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 18s 110ms/step - loss: 0.9109 - accuracy: 0.6038\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 16s 106ms/step - loss: 0.6829 - accuracy: 0.7323\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 16s 107ms/step - loss: 0.4183 - accuracy: 0.8524\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 0.8869 - accuracy: 0.6534\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 17s 96ms/step - loss: 0.9064 - accuracy: 0.5950\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 15s 99ms/step - loss: 0.7517 - accuracy: 0.7011\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 15s 96ms/step - loss: 0.6087 - accuracy: 0.7699\n",
      "76/76 [==============================] - 3s 32ms/step - loss: 0.7984 - accuracy: 0.6643\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 16s 97ms/step - loss: 0.8988 - accuracy: 0.5977\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.7339 - accuracy: 0.7015\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.5774 - accuracy: 0.7854\n",
      "76/76 [==============================] - 3s 32ms/step - loss: 0.8676 - accuracy: 0.6486\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 16s 98ms/step - loss: 0.8957 - accuracy: 0.6011\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 14s 95ms/step - loss: 0.7383 - accuracy: 0.7013\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 15s 96ms/step - loss: 0.5842 - accuracy: 0.7838\n",
      "76/76 [==============================] - 3s 32ms/step - loss: 0.8699 - accuracy: 0.6187\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 21s 132ms/step - loss: 0.9220 - accuracy: 0.5952\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 20s 132ms/step - loss: 0.6894 - accuracy: 0.7298\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 20s 130ms/step - loss: 0.4132 - accuracy: 0.8534\n",
      "76/76 [==============================] - 3s 40ms/step - loss: 0.8662 - accuracy: 0.6573\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 23s 145ms/step - loss: 0.9039 - accuracy: 0.6020\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 21s 139ms/step - loss: 0.6544 - accuracy: 0.7482\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3839 - accuracy: 0.8693\n",
      "76/76 [==============================] - 4s 42ms/step - loss: 0.9554 - accuracy: 0.6449\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 22s 139ms/step - loss: 0.9046 - accuracy: 0.5947\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 21s 139ms/step - loss: 0.6588 - accuracy: 0.7522\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3905 - accuracy: 0.8644\n",
      "76/76 [==============================] - 4s 47ms/step - loss: 0.9413 - accuracy: 0.6534\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 22s 131ms/step - loss: 0.9088 - accuracy: 0.5950\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 20s 128ms/step - loss: 0.7395 - accuracy: 0.7120\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 19s 127ms/step - loss: 0.5775 - accuracy: 0.7813\n",
      "76/76 [==============================] - 4s 42ms/step - loss: 0.8166 - accuracy: 0.6519\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 21s 125ms/step - loss: 0.8889 - accuracy: 0.6062\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 20s 129ms/step - loss: 0.7071 - accuracy: 0.7176\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 20s 129ms/step - loss: 0.5469 - accuracy: 0.8017\n",
      "76/76 [==============================] - 4s 43ms/step - loss: 0.8849 - accuracy: 0.6482\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 21s 127ms/step - loss: 0.8927 - accuracy: 0.6000\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 19s 127ms/step - loss: 0.7253 - accuracy: 0.7177\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 20s 128ms/step - loss: 0.5673 - accuracy: 0.7869\n",
      "76/76 [==============================] - 4s 47ms/step - loss: 0.8682 - accuracy: 0.6410\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 21s 129ms/step - loss: 0.9210 - accuracy: 0.5940\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 20s 131ms/step - loss: 0.6937 - accuracy: 0.7277\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 20s 133ms/step - loss: 0.4301 - accuracy: 0.8454\n",
      "76/76 [==============================] - 4s 41ms/step - loss: 0.9031 - accuracy: 0.6573\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 21s 132ms/step - loss: 0.9084 - accuracy: 0.6018\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 20s 133ms/step - loss: 0.6631 - accuracy: 0.7364\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 21s 135ms/step - loss: 0.4021 - accuracy: 0.8594\n",
      "76/76 [==============================] - 4s 44ms/step - loss: 0.9706 - accuracy: 0.6255\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 22s 132ms/step - loss: 0.9194 - accuracy: 0.5854\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 20s 134ms/step - loss: 0.6817 - accuracy: 0.7350\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 20s 135ms/step - loss: 0.4150 - accuracy: 0.8475\n",
      "76/76 [==============================] - 3s 40ms/step - loss: 0.9357 - accuracy: 0.6439\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 20s 124ms/step - loss: 0.9007 - accuracy: 0.6012\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 18s 122ms/step - loss: 0.7554 - accuracy: 0.6982\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.6084 - accuracy: 0.7676\n",
      "76/76 [==============================] - 3s 41ms/step - loss: 0.9448 - accuracy: 0.5523\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 20s 119ms/step - loss: 0.8904 - accuracy: 0.6078\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.7182 - accuracy: 0.7114\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.5686 - accuracy: 0.7850\n",
      "76/76 [==============================] - 3s 41ms/step - loss: 0.8823 - accuracy: 0.6437\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 20s 121ms/step - loss: 0.8898 - accuracy: 0.6083\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.7369 - accuracy: 0.7040\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 18s 121ms/step - loss: 0.5942 - accuracy: 0.7722\n",
      "76/76 [==============================] - 4s 42ms/step - loss: 0.8268 - accuracy: 0.6551\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 28s 178ms/step - loss: 0.9086 - accuracy: 0.6004\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 28s 181ms/step - loss: 0.6594 - accuracy: 0.7465\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 28s 181ms/step - loss: 0.3884 - accuracy: 0.8569\n",
      "76/76 [==============================] - 6s 67ms/step - loss: 0.9532 - accuracy: 0.6428\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 29s 177ms/step - loss: 0.8948 - accuracy: 0.6062\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 27s 177ms/step - loss: 0.6369 - accuracy: 0.7503\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 28s 182ms/step - loss: 0.3741 - accuracy: 0.8654\n",
      "76/76 [==============================] - 5s 65ms/step - loss: 0.9777 - accuracy: 0.6457\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 28s 174ms/step - loss: 0.9010 - accuracy: 0.6033\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 27s 179ms/step - loss: 0.6475 - accuracy: 0.7484\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 27s 177ms/step - loss: 0.3823 - accuracy: 0.8621\n",
      "76/76 [==============================] - 5s 68ms/step - loss: 0.9752 - accuracy: 0.6361\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 27s 168ms/step - loss: 0.9030 - accuracy: 0.5969\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 26s 172ms/step - loss: 0.7281 - accuracy: 0.7126\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 26s 171ms/step - loss: 0.5671 - accuracy: 0.7804\n",
      "76/76 [==============================] - 6s 69ms/step - loss: 0.8062 - accuracy: 0.6577\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 27s 169ms/step - loss: 0.8858 - accuracy: 0.6016\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 26s 169ms/step - loss: 0.6998 - accuracy: 0.7228\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 25s 166ms/step - loss: 0.5384 - accuracy: 0.8040\n",
      "76/76 [==============================] - 6s 69ms/step - loss: 0.8648 - accuracy: 0.6548\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 27s 167ms/step - loss: 0.8870 - accuracy: 0.6069\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 26s 169ms/step - loss: 0.7093 - accuracy: 0.7170\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 26s 168ms/step - loss: 0.5501 - accuracy: 0.7914\n",
      "76/76 [==============================] - 5s 65ms/step - loss: 0.8455 - accuracy: 0.6406\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 42s 270ms/step - loss: 0.8986 - accuracy: 0.6107\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 42s 276ms/step - loss: 0.6390 - accuracy: 0.7492\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 41s 267ms/step - loss: 0.3720 - accuracy: 0.8640\n",
      "76/76 [==============================] - 6s 80ms/step - loss: 0.9347 - accuracy: 0.6548\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 43s 273ms/step - loss: 0.8902 - accuracy: 0.6068\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 41s 271ms/step - loss: 0.6173 - accuracy: 0.7614\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 41s 269ms/step - loss: 0.3462 - accuracy: 0.8778\n",
      "76/76 [==============================] - 6s 81ms/step - loss: 1.0021 - accuracy: 0.6358\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 42s 268ms/step - loss: 0.8928 - accuracy: 0.6067\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 41s 267ms/step - loss: 0.6205 - accuracy: 0.7644\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 41s 270ms/step - loss: 0.3603 - accuracy: 0.8714\n",
      "76/76 [==============================] - 7s 83ms/step - loss: 0.9334 - accuracy: 0.6543\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 40s 253ms/step - loss: 0.9044 - accuracy: 0.5944\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 36s 238ms/step - loss: 0.7208 - accuracy: 0.7164\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 36s 239ms/step - loss: 0.5501 - accuracy: 0.7910\n",
      "76/76 [==============================] - 6s 77ms/step - loss: 0.8296 - accuracy: 0.6432\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 39s 242ms/step - loss: 0.8799 - accuracy: 0.6103\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 37s 241ms/step - loss: 0.6854 - accuracy: 0.7250\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 36s 239ms/step - loss: 0.5173 - accuracy: 0.8160\n",
      "76/76 [==============================] - 6s 76ms/step - loss: 0.9819 - accuracy: 0.6461\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 38s 243ms/step - loss: 0.8856 - accuracy: 0.6071\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 40s 263ms/step - loss: 0.6935 - accuracy: 0.7274\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 38s 253ms/step - loss: 0.5277 - accuracy: 0.8034\n",
      "76/76 [==============================] - 6s 79ms/step - loss: 0.8669 - accuracy: 0.6460\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 12s 67ms/step - loss: 0.9481 - accuracy: 0.5754\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.7384 - accuracy: 0.7122\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.4938 - accuracy: 0.8249\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.3066 - accuracy: 0.8946\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2112 - accuracy: 0.9202\n",
      "76/76 [==============================] - 2s 15ms/step - loss: 1.0307 - accuracy: 0.6449\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 12s 65ms/step - loss: 0.9169 - accuracy: 0.5948\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.7116 - accuracy: 0.7327\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 10s 66ms/step - loss: 0.4806 - accuracy: 0.8294\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.3065 - accuracy: 0.8944\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.2046 - accuracy: 0.9305\n",
      "76/76 [==============================] - 2s 15ms/step - loss: 1.0746 - accuracy: 0.6519\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 11s 65ms/step - loss: 0.9154 - accuracy: 0.5938\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 10s 66ms/step - loss: 0.7263 - accuracy: 0.7125\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 10s 63ms/step - loss: 0.4931 - accuracy: 0.8206\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 10s 63ms/step - loss: 0.3174 - accuracy: 0.8896\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2164 - accuracy: 0.9225\n",
      "76/76 [==============================] - 1s 13ms/step - loss: 1.0410 - accuracy: 0.6505\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 8s 47ms/step - loss: 0.9228 - accuracy: 0.5927\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.7851 - accuracy: 0.6932\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.6602 - accuracy: 0.7563\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.5472 - accuracy: 0.7988\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 7s 48ms/step - loss: 0.4466 - accuracy: 0.8425\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 0.8436 - accuracy: 0.6652\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 9s 48ms/step - loss: 0.9137 - accuracy: 0.5964\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 7s 47ms/step - loss: 0.7712 - accuracy: 0.6884\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 7s 48ms/step - loss: 0.6324 - accuracy: 0.7689\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 8s 51ms/step - loss: 0.5142 - accuracy: 0.8253\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.4222 - accuracy: 0.8588\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 0.9091 - accuracy: 0.6482\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 9s 47ms/step - loss: 0.9093 - accuracy: 0.5949\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.7806 - accuracy: 0.6703\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.6502 - accuracy: 0.7555\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.5285 - accuracy: 0.8121\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.4288 - accuracy: 0.8520\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 0.8457 - accuracy: 0.6646\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 14s 80ms/step - loss: 0.9257 - accuracy: 0.5892\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 12s 79ms/step - loss: 0.7245 - accuracy: 0.7195\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 12s 80ms/step - loss: 0.4672 - accuracy: 0.8290\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 12s 77ms/step - loss: 0.2883 - accuracy: 0.8968\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 12s 77ms/step - loss: 0.1976 - accuracy: 0.9289\n",
      "76/76 [==============================] - 2s 19ms/step - loss: 1.0326 - accuracy: 0.6527\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 13s 78ms/step - loss: 0.9258 - accuracy: 0.5896\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 12s 78ms/step - loss: 0.7040 - accuracy: 0.7252\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 12s 77ms/step - loss: 0.4391 - accuracy: 0.8470\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 12s 79ms/step - loss: 0.2684 - accuracy: 0.9051\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 12s 78ms/step - loss: 0.1774 - accuracy: 0.9405\n",
      "76/76 [==============================] - 2s 20ms/step - loss: 1.0867 - accuracy: 0.6391\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 13s 77ms/step - loss: 0.9299 - accuracy: 0.5831\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 12s 76ms/step - loss: 0.7192 - accuracy: 0.7197\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 12s 77ms/step - loss: 0.4569 - accuracy: 0.8353\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 12s 78ms/step - loss: 0.2766 - accuracy: 0.9060\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 12s 77ms/step - loss: 0.1866 - accuracy: 0.9312\n",
      "76/76 [==============================] - 2s 19ms/step - loss: 1.0576 - accuracy: 0.6526\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 11s 66ms/step - loss: 0.9138 - accuracy: 0.5915\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 10s 66ms/step - loss: 0.7635 - accuracy: 0.7037\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.6258 - accuracy: 0.7639\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 10s 68ms/step - loss: 0.5080 - accuracy: 0.8166\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 10s 66ms/step - loss: 0.4101 - accuracy: 0.8582\n",
      "76/76 [==============================] - 2s 20ms/step - loss: 0.8582 - accuracy: 0.6589\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 12s 66ms/step - loss: 0.9032 - accuracy: 0.6000\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.7511 - accuracy: 0.6924\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 10s 66ms/step - loss: 0.5990 - accuracy: 0.7788\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 10s 66ms/step - loss: 0.4751 - accuracy: 0.8294\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 10s 66ms/step - loss: 0.3780 - accuracy: 0.8683\n",
      "76/76 [==============================] - 2s 21ms/step - loss: 0.9179 - accuracy: 0.6494\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 12s 70ms/step - loss: 0.9127 - accuracy: 0.5905\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 10s 67ms/step - loss: 0.7638 - accuracy: 0.6842\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 10s 66ms/step - loss: 0.6126 - accuracy: 0.7689\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 10s 66ms/step - loss: 0.4846 - accuracy: 0.8231\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 10s 67ms/step - loss: 0.3872 - accuracy: 0.8625\n",
      "76/76 [==============================] - 2s 18ms/step - loss: 0.9218 - accuracy: 0.6356\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 15s 89ms/step - loss: 0.9300 - accuracy: 0.5884\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 13s 87ms/step - loss: 0.7073 - accuracy: 0.7209\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 14s 90ms/step - loss: 0.4406 - accuracy: 0.8445\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 14s 89ms/step - loss: 0.2811 - accuracy: 0.9008\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 14s 89ms/step - loss: 0.1968 - accuracy: 0.9256\n",
      "76/76 [==============================] - 2s 22ms/step - loss: 1.0406 - accuracy: 0.6515\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 15s 89ms/step - loss: 0.9284 - accuracy: 0.5931\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 14s 90ms/step - loss: 0.6886 - accuracy: 0.7383\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 13s 88ms/step - loss: 0.4273 - accuracy: 0.8540\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 13s 88ms/step - loss: 0.2610 - accuracy: 0.9101\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 14s 90ms/step - loss: 0.1722 - accuracy: 0.9419\n",
      "76/76 [==============================] - 2s 23ms/step - loss: 1.1218 - accuracy: 0.6346\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 15s 89ms/step - loss: 0.9158 - accuracy: 0.5959\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.6929 - accuracy: 0.7350\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 14s 92ms/step - loss: 0.4241 - accuracy: 0.8487\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2640 - accuracy: 0.9039\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1741 - accuracy: 0.9359\n",
      "76/76 [==============================] - 2s 24ms/step - loss: 1.0721 - accuracy: 0.6522\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 14s 82ms/step - loss: 0.9149 - accuracy: 0.5900\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 12s 80ms/step - loss: 0.7635 - accuracy: 0.7004\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 12s 80ms/step - loss: 0.6139 - accuracy: 0.7728\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 13s 83ms/step - loss: 0.4895 - accuracy: 0.8228\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 12s 82ms/step - loss: 0.3887 - accuracy: 0.8667\n",
      "76/76 [==============================] - 2s 24ms/step - loss: 0.8596 - accuracy: 0.6730\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 13s 79ms/step - loss: 0.9076 - accuracy: 0.5956\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 12s 78ms/step - loss: 0.7480 - accuracy: 0.7002\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 12s 81ms/step - loss: 0.5922 - accuracy: 0.7794\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 12s 80ms/step - loss: 0.4667 - accuracy: 0.8346\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 12s 80ms/step - loss: 0.3672 - accuracy: 0.8724\n",
      "76/76 [==============================] - 2s 24ms/step - loss: 0.9396 - accuracy: 0.6403\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 13s 79ms/step - loss: 0.9025 - accuracy: 0.5974\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 12s 80ms/step - loss: 0.7447 - accuracy: 0.7001\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 12s 81ms/step - loss: 0.5962 - accuracy: 0.7790\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 12s 80ms/step - loss: 0.4747 - accuracy: 0.8322\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 12s 79ms/step - loss: 0.3738 - accuracy: 0.8687\n",
      "76/76 [==============================] - 2s 24ms/step - loss: 0.9110 - accuracy: 0.6605\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 14s 83ms/step - loss: 0.9369 - accuracy: 0.5871\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 13s 83ms/step - loss: 0.7292 - accuracy: 0.7145\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 13s 85ms/step - loss: 0.4761 - accuracy: 0.8257\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 13s 84ms/step - loss: 0.2893 - accuracy: 0.8964\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 13s 85ms/step - loss: 0.2053 - accuracy: 0.9227\n",
      "76/76 [==============================] - 2s 22ms/step - loss: 1.0752 - accuracy: 0.6449\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 14s 86ms/step - loss: 0.9179 - accuracy: 0.5977\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 13s 85ms/step - loss: 0.6971 - accuracy: 0.7300\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 13s 83ms/step - loss: 0.4511 - accuracy: 0.8379\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 13s 82ms/step - loss: 0.2805 - accuracy: 0.9016\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 15s 95ms/step - loss: 0.1892 - accuracy: 0.9320\n",
      "76/76 [==============================] - 2s 23ms/step - loss: 1.1333 - accuracy: 0.6445\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 15s 90ms/step - loss: 0.9197 - accuracy: 0.5901\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 14s 90ms/step - loss: 0.7074 - accuracy: 0.7205\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 14s 89ms/step - loss: 0.4589 - accuracy: 0.8348\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 14s 92ms/step - loss: 0.2854 - accuracy: 0.8921\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1925 - accuracy: 0.9274\n",
      "76/76 [==============================] - 2s 24ms/step - loss: 1.0769 - accuracy: 0.6625\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 13s 79ms/step - loss: 0.9158 - accuracy: 0.5923\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 12s 79ms/step - loss: 0.7664 - accuracy: 0.6963\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 12s 79ms/step - loss: 0.6348 - accuracy: 0.7616\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 12s 79ms/step - loss: 0.5146 - accuracy: 0.8127\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 12s 79ms/step - loss: 0.4163 - accuracy: 0.8505\n",
      "76/76 [==============================] - 2s 24ms/step - loss: 0.8573 - accuracy: 0.6544\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 14s 79ms/step - loss: 0.8910 - accuracy: 0.6039\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 12s 78ms/step - loss: 0.7386 - accuracy: 0.7002\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 12s 78ms/step - loss: 0.5981 - accuracy: 0.7740\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 12s 77ms/step - loss: 0.4835 - accuracy: 0.8311\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 12s 78ms/step - loss: 0.3901 - accuracy: 0.8658\n",
      "76/76 [==============================] - 2s 23ms/step - loss: 0.9352 - accuracy: 0.6478\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 14s 80ms/step - loss: 0.9090 - accuracy: 0.5963\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 12s 78ms/step - loss: 0.7568 - accuracy: 0.6945\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 12s 82ms/step - loss: 0.6185 - accuracy: 0.7650\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 13s 82ms/step - loss: 0.4996 - accuracy: 0.8210\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 12s 81ms/step - loss: 0.4044 - accuracy: 0.8561\n",
      "76/76 [==============================] - 2s 25ms/step - loss: 0.8811 - accuracy: 0.6510\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 19s 117ms/step - loss: 0.9260 - accuracy: 0.5915\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.6925 - accuracy: 0.7360\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.4289 - accuracy: 0.8468\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 17s 112ms/step - loss: 0.2706 - accuracy: 0.9061\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 17s 112ms/step - loss: 0.1949 - accuracy: 0.9278\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 1.0783 - accuracy: 0.6544\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 19s 113ms/step - loss: 0.9185 - accuracy: 0.6024\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.6672 - accuracy: 0.7418\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4014 - accuracy: 0.8621\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2476 - accuracy: 0.9117\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 17s 112ms/step - loss: 0.1634 - accuracy: 0.9403\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 1.2208 - accuracy: 0.6300\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 19s 114ms/step - loss: 0.9199 - accuracy: 0.5957\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.6809 - accuracy: 0.7360\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.4233 - accuracy: 0.8499\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.2627 - accuracy: 0.9033\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1763 - accuracy: 0.9337\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 1.1691 - accuracy: 0.6319\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 18s 108ms/step - loss: 0.9072 - accuracy: 0.5948\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 16s 103ms/step - loss: 0.7539 - accuracy: 0.7023\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 16s 104ms/step - loss: 0.6009 - accuracy: 0.7711\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 16s 103ms/step - loss: 0.4780 - accuracy: 0.8234\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 16s 104ms/step - loss: 0.3774 - accuracy: 0.8671\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.9027 - accuracy: 0.6461\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 17s 103ms/step - loss: 0.8951 - accuracy: 0.6041\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 15s 101ms/step - loss: 0.7197 - accuracy: 0.7114\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 16s 103ms/step - loss: 0.5616 - accuracy: 0.7949\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 16s 104ms/step - loss: 0.4420 - accuracy: 0.8418\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 16s 103ms/step - loss: 0.3464 - accuracy: 0.8815\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.9581 - accuracy: 0.6565\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 17s 102ms/step - loss: 0.8942 - accuracy: 0.6033\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 16s 103ms/step - loss: 0.7331 - accuracy: 0.7065\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 16s 105ms/step - loss: 0.5936 - accuracy: 0.7774\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 16s 103ms/step - loss: 0.4698 - accuracy: 0.8282\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 16s 103ms/step - loss: 0.3719 - accuracy: 0.8687\n",
      "76/76 [==============================] - 3s 36ms/step - loss: 0.9536 - accuracy: 0.6228\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 22s 139ms/step - loss: 0.9126 - accuracy: 0.5971\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 21s 138ms/step - loss: 0.6658 - accuracy: 0.7426\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 0.4039 - accuracy: 0.8590\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 21s 139ms/step - loss: 0.2478 - accuracy: 0.9092\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 20s 134ms/step - loss: 0.1699 - accuracy: 0.9374\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 1.0899 - accuracy: 0.6478\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 21s 129ms/step - loss: 0.9035 - accuracy: 0.6043\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 20s 130ms/step - loss: 0.6450 - accuracy: 0.7511\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 21s 136ms/step - loss: 0.3765 - accuracy: 0.8658\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 20s 132ms/step - loss: 0.2219 - accuracy: 0.9206\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 21s 138ms/step - loss: 0.1466 - accuracy: 0.9467\n",
      "76/76 [==============================] - 4s 40ms/step - loss: 1.1770 - accuracy: 0.6296\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 21s 126ms/step - loss: 0.9087 - accuracy: 0.5969\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 19s 126ms/step - loss: 0.6693 - accuracy: 0.7391\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 19s 128ms/step - loss: 0.4065 - accuracy: 0.8576\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 19s 127ms/step - loss: 0.2400 - accuracy: 0.9117\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 20s 130ms/step - loss: 0.1628 - accuracy: 0.9396\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 1.1654 - accuracy: 0.6613\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 20s 120ms/step - loss: 0.9125 - accuracy: 0.5969\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 18s 121ms/step - loss: 0.7412 - accuracy: 0.7066\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.5771 - accuracy: 0.7835\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 20s 130ms/step - loss: 0.4471 - accuracy: 0.8381\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.3460 - accuracy: 0.8760\n",
      "76/76 [==============================] - 4s 44ms/step - loss: 0.9280 - accuracy: 0.6242\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 20s 121ms/step - loss: 0.8918 - accuracy: 0.6018\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 18s 121ms/step - loss: 0.7097 - accuracy: 0.7147\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.5478 - accuracy: 0.7990\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 18s 121ms/step - loss: 0.4209 - accuracy: 0.8520\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.3273 - accuracy: 0.8813\n",
      "76/76 [==============================] - 3s 41ms/step - loss: 0.9659 - accuracy: 0.6403\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 23s 136ms/step - loss: 0.8906 - accuracy: 0.6038\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 20s 132ms/step - loss: 0.7257 - accuracy: 0.7129\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 20s 132ms/step - loss: 0.5754 - accuracy: 0.7859\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 20s 134ms/step - loss: 0.4478 - accuracy: 0.8421\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 20s 134ms/step - loss: 0.3423 - accuracy: 0.8818\n",
      "76/76 [==============================] - 4s 51ms/step - loss: 0.9369 - accuracy: 0.6567\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 22s 135ms/step - loss: 0.9138 - accuracy: 0.5987\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.7016 - accuracy: 0.7244\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 21s 135ms/step - loss: 0.4374 - accuracy: 0.8410\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 20s 133ms/step - loss: 0.2649 - accuracy: 0.8999\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 20s 134ms/step - loss: 0.1927 - accuracy: 0.9281\n",
      "76/76 [==============================] - 4s 42ms/step - loss: 1.1593 - accuracy: 0.6523\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 22s 134ms/step - loss: 0.9005 - accuracy: 0.6111\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 20s 134ms/step - loss: 0.6682 - accuracy: 0.7407\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 20s 131ms/step - loss: 0.4122 - accuracy: 0.8507\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 20s 133ms/step - loss: 0.2518 - accuracy: 0.9088\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 20s 133ms/step - loss: 0.1787 - accuracy: 0.9353\n",
      "76/76 [==============================] - 4s 46ms/step - loss: 1.2220 - accuracy: 0.6428\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 22s 135ms/step - loss: 0.9044 - accuracy: 0.6017\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 21s 136ms/step - loss: 0.6829 - accuracy: 0.7336\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 20s 134ms/step - loss: 0.4210 - accuracy: 0.8508\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 20s 134ms/step - loss: 0.2654 - accuracy: 0.8987\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 21s 136ms/step - loss: 0.1874 - accuracy: 0.9291\n",
      "76/76 [==============================] - 4s 45ms/step - loss: 1.1526 - accuracy: 0.6443\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 21s 127ms/step - loss: 0.9072 - accuracy: 0.5989\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 19s 125ms/step - loss: 0.7548 - accuracy: 0.6961\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 19s 128ms/step - loss: 0.6003 - accuracy: 0.7676\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 19s 127ms/step - loss: 0.4764 - accuracy: 0.8239\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 19s 126ms/step - loss: 0.3793 - accuracy: 0.8631\n",
      "76/76 [==============================] - 4s 42ms/step - loss: 0.9246 - accuracy: 0.6747\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 20s 125ms/step - loss: 0.8874 - accuracy: 0.6084\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 19s 125ms/step - loss: 0.7232 - accuracy: 0.7099\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 19s 125ms/step - loss: 0.5797 - accuracy: 0.7877\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.4604 - accuracy: 0.8313\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.3648 - accuracy: 0.8718\n",
      "76/76 [==============================] - 4s 44ms/step - loss: 0.9685 - accuracy: 0.6358\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 20s 122ms/step - loss: 0.8965 - accuracy: 0.6031\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 19s 125ms/step - loss: 0.7382 - accuracy: 0.7032\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.5901 - accuracy: 0.7784\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 19s 126ms/step - loss: 0.4691 - accuracy: 0.8307\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 19s 125ms/step - loss: 0.3762 - accuracy: 0.8671\n",
      "76/76 [==============================] - 4s 49ms/step - loss: 0.9250 - accuracy: 0.6638\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 31s 188ms/step - loss: 0.9101 - accuracy: 0.6035\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 29s 189ms/step - loss: 0.6675 - accuracy: 0.7376\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 29s 193ms/step - loss: 0.3901 - accuracy: 0.8538\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 29s 192ms/step - loss: 0.2449 - accuracy: 0.9088\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 29s 188ms/step - loss: 0.1687 - accuracy: 0.9357\n",
      "76/76 [==============================] - 6s 72ms/step - loss: 1.1313 - accuracy: 0.6494\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 30s 188ms/step - loss: 0.8971 - accuracy: 0.6051\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 28s 187ms/step - loss: 0.6302 - accuracy: 0.7598\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 29s 189ms/step - loss: 0.3652 - accuracy: 0.8687\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 29s 192ms/step - loss: 0.2245 - accuracy: 0.9181\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 28s 181ms/step - loss: 0.1497 - accuracy: 0.9460\n",
      "76/76 [==============================] - 6s 79ms/step - loss: 1.2836 - accuracy: 0.6362\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 30s 187ms/step - loss: 0.8947 - accuracy: 0.6073\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 28s 182ms/step - loss: 0.6408 - accuracy: 0.7549\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.3825 - accuracy: 0.8636\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.2344 - accuracy: 0.9130\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.1571 - accuracy: 0.9415\n",
      "76/76 [==============================] - 5s 64ms/step - loss: 1.2441 - accuracy: 0.6340\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 26s 162ms/step - loss: 0.8986 - accuracy: 0.5987\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 25s 165ms/step - loss: 0.7299 - accuracy: 0.7099\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 25s 165ms/step - loss: 0.5678 - accuracy: 0.7852\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 25s 162ms/step - loss: 0.4422 - accuracy: 0.8369\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 25s 163ms/step - loss: 0.3424 - accuracy: 0.8768\n",
      "76/76 [==============================] - 5s 65ms/step - loss: 0.9341 - accuracy: 0.6536\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 26s 164ms/step - loss: 0.8841 - accuracy: 0.6035\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 25s 161ms/step - loss: 0.6859 - accuracy: 0.7308\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 25s 161ms/step - loss: 0.5198 - accuracy: 0.8075\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 25s 162ms/step - loss: 0.4005 - accuracy: 0.8540\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 25s 161ms/step - loss: 0.3086 - accuracy: 0.8917\n",
      "76/76 [==============================] - 5s 62ms/step - loss: 1.0427 - accuracy: 0.6445\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 27s 159ms/step - loss: 0.8837 - accuracy: 0.6085\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 24s 159ms/step - loss: 0.7079 - accuracy: 0.7218\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 25s 164ms/step - loss: 0.5523 - accuracy: 0.7914\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 25s 161ms/step - loss: 0.4288 - accuracy: 0.8419\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 24s 158ms/step - loss: 0.3358 - accuracy: 0.8752\n",
      "76/76 [==============================] - 5s 63ms/step - loss: 1.0042 - accuracy: 0.6203\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 40s 252ms/step - loss: 0.9066 - accuracy: 0.6088\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 39s 258ms/step - loss: 0.6501 - accuracy: 0.7445\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 39s 254ms/step - loss: 0.3779 - accuracy: 0.8646\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 39s 258ms/step - loss: 0.2321 - accuracy: 0.9136\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 41s 270ms/step - loss: 0.1544 - accuracy: 0.9411\n",
      "76/76 [==============================] - 7s 89ms/step - loss: 1.1311 - accuracy: 0.6503\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 45s 287ms/step - loss: 0.8863 - accuracy: 0.6062\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 43s 283ms/step - loss: 0.6225 - accuracy: 0.7571\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 44s 287ms/step - loss: 0.3565 - accuracy: 0.8741\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 44s 291ms/step - loss: 0.2127 - accuracy: 0.9210\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 43s 283ms/step - loss: 0.1453 - accuracy: 0.9489\n",
      "76/76 [==============================] - 7s 87ms/step - loss: 1.2122 - accuracy: 0.6432\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 45s 282ms/step - loss: 0.8927 - accuracy: 0.6077\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 43s 282ms/step - loss: 0.6181 - accuracy: 0.7625\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 43s 284ms/step - loss: 0.3642 - accuracy: 0.8714\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 43s 282ms/step - loss: 0.2231 - accuracy: 0.9157\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 43s 283ms/step - loss: 0.1559 - accuracy: 0.9423\n",
      "76/76 [==============================] - 7s 87ms/step - loss: 1.2038 - accuracy: 0.6390\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 45s 285ms/step - loss: 0.8958 - accuracy: 0.5989\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 42s 277ms/step - loss: 0.7124 - accuracy: 0.7236\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 42s 275ms/step - loss: 0.5447 - accuracy: 0.7941\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 43s 284ms/step - loss: 0.4092 - accuracy: 0.8509\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 43s 281ms/step - loss: 0.3129 - accuracy: 0.8890\n",
      "76/76 [==============================] - 7s 90ms/step - loss: 0.9735 - accuracy: 0.6734\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 44s 278ms/step - loss: 0.8800 - accuracy: 0.6113\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 42s 275ms/step - loss: 0.6761 - accuracy: 0.7317\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 42s 276ms/step - loss: 0.5058 - accuracy: 0.8133\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 41s 271ms/step - loss: 0.3811 - accuracy: 0.8633\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 41s 270ms/step - loss: 0.2883 - accuracy: 0.8952\n",
      "76/76 [==============================] - 7s 81ms/step - loss: 1.0070 - accuracy: 0.6317\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 41s 259ms/step - loss: 0.8843 - accuracy: 0.6081\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 38s 250ms/step - loss: 0.6973 - accuracy: 0.7245\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 39s 258ms/step - loss: 0.5286 - accuracy: 0.8049\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 39s 258ms/step - loss: 0.3982 - accuracy: 0.8590\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 40s 261ms/step - loss: 0.2980 - accuracy: 0.8948\n",
      "76/76 [==============================] - 7s 81ms/step - loss: 0.9532 - accuracy: 0.6638\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 9s 98ms/step - loss: 0.9807 - accuracy: 0.5727\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 8s 103ms/step - loss: 0.7990 - accuracy: 0.6742\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 7s 96ms/step - loss: 0.6252 - accuracy: 0.7736\n",
      "38/38 [==============================] - 1s 27ms/step - loss: 0.7846 - accuracy: 0.6776\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 9s 101ms/step - loss: 0.9916 - accuracy: 0.5776\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 7s 97ms/step - loss: 0.7959 - accuracy: 0.6630\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 7s 94ms/step - loss: 0.6101 - accuracy: 0.7852\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.8402 - accuracy: 0.6507\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 9s 99ms/step - loss: 0.9834 - accuracy: 0.5682\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 7s 96ms/step - loss: 0.7858 - accuracy: 0.6842\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 7s 94ms/step - loss: 0.6102 - accuracy: 0.7865\n",
      "38/38 [==============================] - 1s 27ms/step - loss: 0.8190 - accuracy: 0.6567\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 8s 86ms/step - loss: 0.9561 - accuracy: 0.5797\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 6s 85ms/step - loss: 0.8231 - accuracy: 0.6595\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 7s 88ms/step - loss: 0.7075 - accuracy: 0.7385\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.7856 - accuracy: 0.6561\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 8s 85ms/step - loss: 0.9442 - accuracy: 0.5861\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 7s 86ms/step - loss: 0.8017 - accuracy: 0.6630\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 7s 87ms/step - loss: 0.6802 - accuracy: 0.7467\n",
      "38/38 [==============================] - 2s 29ms/step - loss: 0.8168 - accuracy: 0.6490\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 8s 85ms/step - loss: 0.9471 - accuracy: 0.5891\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 6s 84ms/step - loss: 0.8108 - accuracy: 0.6554\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 7s 86ms/step - loss: 0.6973 - accuracy: 0.7360\n",
      "38/38 [==============================] - 3s 29ms/step - loss: 0.8170 - accuracy: 0.6456\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 11s 121ms/step - loss: 0.9651 - accuracy: 0.5826\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.7952 - accuracy: 0.6822\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.5956 - accuracy: 0.7806\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 0.8015 - accuracy: 0.6676\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 11s 122ms/step - loss: 0.9727 - accuracy: 0.5900\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 9s 121ms/step - loss: 0.7786 - accuracy: 0.6779\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.5662 - accuracy: 0.8007\n",
      "38/38 [==============================] - 2s 34ms/step - loss: 0.8500 - accuracy: 0.6457\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 11s 121ms/step - loss: 0.9936 - accuracy: 0.5688\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.8043 - accuracy: 0.6571\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 9s 121ms/step - loss: 0.5965 - accuracy: 0.7832\n",
      "38/38 [==============================] - 2s 35ms/step - loss: 0.8256 - accuracy: 0.6592\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 11s 116ms/step - loss: 0.9350 - accuracy: 0.5816\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 9s 115ms/step - loss: 0.7982 - accuracy: 0.6808\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 9s 113ms/step - loss: 0.6643 - accuracy: 0.7534\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 0.7848 - accuracy: 0.6614\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 10s 116ms/step - loss: 0.9264 - accuracy: 0.5958\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 9s 116ms/step - loss: 0.7933 - accuracy: 0.6744\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 9s 112ms/step - loss: 0.6560 - accuracy: 0.7538\n",
      "38/38 [==============================] - 2s 38ms/step - loss: 0.8312 - accuracy: 0.6540\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 10s 114ms/step - loss: 0.9334 - accuracy: 0.5887\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 9s 112ms/step - loss: 0.8022 - accuracy: 0.6604\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 9s 113ms/step - loss: 0.6747 - accuracy: 0.7451\n",
      "38/38 [==============================] - 2s 37ms/step - loss: 0.8117 - accuracy: 0.6584\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 13s 152ms/step - loss: 0.9737 - accuracy: 0.5754\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 12s 151ms/step - loss: 0.7908 - accuracy: 0.6880\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 12s 155ms/step - loss: 0.5782 - accuracy: 0.7937\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.8164 - accuracy: 0.6548\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 13s 146ms/step - loss: 0.9864 - accuracy: 0.5880\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 11s 147ms/step - loss: 0.7814 - accuracy: 0.6800\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 13s 165ms/step - loss: 0.5542 - accuracy: 0.8026\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.8682 - accuracy: 0.6490\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 13s 151ms/step - loss: 0.9568 - accuracy: 0.5852\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 11s 148ms/step - loss: 0.7760 - accuracy: 0.6784\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 11s 147ms/step - loss: 0.5529 - accuracy: 0.8014\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.8366 - accuracy: 0.6427\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 12s 139ms/step - loss: 0.9397 - accuracy: 0.5799\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 11s 140ms/step - loss: 0.8074 - accuracy: 0.6777\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 0.6678 - accuracy: 0.7579\n",
      "38/38 [==============================] - 2s 43ms/step - loss: 0.7984 - accuracy: 0.6577\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 12s 141ms/step - loss: 0.9132 - accuracy: 0.5967\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 11s 138ms/step - loss: 0.7907 - accuracy: 0.6752\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 0.6478 - accuracy: 0.7664\n",
      "38/38 [==============================] - 2s 43ms/step - loss: 0.8422 - accuracy: 0.6441\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 12s 140ms/step - loss: 0.9266 - accuracy: 0.5843\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 0.8004 - accuracy: 0.6641\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 0.6551 - accuracy: 0.7569\n",
      "38/38 [==============================] - 2s 43ms/step - loss: 0.8096 - accuracy: 0.6526\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 12s 133ms/step - loss: 0.9712 - accuracy: 0.5766\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 0.7912 - accuracy: 0.6725\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 10s 133ms/step - loss: 0.5866 - accuracy: 0.7889\n",
      "38/38 [==============================] - 2s 41ms/step - loss: 0.7995 - accuracy: 0.6680\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 12s 131ms/step - loss: 0.9646 - accuracy: 0.5789\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 10s 134ms/step - loss: 0.7648 - accuracy: 0.6946\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 10s 135ms/step - loss: 0.5561 - accuracy: 0.8026\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.8655 - accuracy: 0.6478\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 12s 133ms/step - loss: 0.9861 - accuracy: 0.5771\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 10s 134ms/step - loss: 0.7782 - accuracy: 0.6902\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 10s 134ms/step - loss: 0.5853 - accuracy: 0.7888\n",
      "38/38 [==============================] - 2s 41ms/step - loss: 0.8389 - accuracy: 0.6468\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 11s 124ms/step - loss: 0.9426 - accuracy: 0.5807\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 9s 124ms/step - loss: 0.7991 - accuracy: 0.6742\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.6679 - accuracy: 0.7484\n",
      "38/38 [==============================] - 2s 43ms/step - loss: 0.7780 - accuracy: 0.6668\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 11s 127ms/step - loss: 0.9203 - accuracy: 0.5944\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.7724 - accuracy: 0.6835\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.6345 - accuracy: 0.7575\n",
      "38/38 [==============================] - 2s 43ms/step - loss: 0.8381 - accuracy: 0.6536\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 11s 123ms/step - loss: 0.9266 - accuracy: 0.5899\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.7835 - accuracy: 0.6790\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.6627 - accuracy: 0.7538\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.8095 - accuracy: 0.6592\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 15s 178ms/step - loss: 0.9638 - accuracy: 0.5805\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 14s 179ms/step - loss: 0.7629 - accuracy: 0.7021\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 14s 179ms/step - loss: 0.5358 - accuracy: 0.7988\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.8212 - accuracy: 0.6714\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 15s 181ms/step - loss: 0.9486 - accuracy: 0.5847\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 14s 179ms/step - loss: 0.7431 - accuracy: 0.7128\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 14s 180ms/step - loss: 0.5150 - accuracy: 0.8164\n",
      "38/38 [==============================] - 3s 66ms/step - loss: 0.8812 - accuracy: 0.6490\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 15s 180ms/step - loss: 0.9453 - accuracy: 0.5899\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 14s 183ms/step - loss: 0.7572 - accuracy: 0.6924\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 14s 180ms/step - loss: 0.5233 - accuracy: 0.8169\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.8503 - accuracy: 0.6543\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 15s 171ms/step - loss: 0.9295 - accuracy: 0.5836\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 13s 171ms/step - loss: 0.7793 - accuracy: 0.6868\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 0.6271 - accuracy: 0.7622\n",
      "38/38 [==============================] - 3s 66ms/step - loss: 0.7941 - accuracy: 0.6693\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 14s 172ms/step - loss: 0.9186 - accuracy: 0.5967\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 13s 171ms/step - loss: 0.7566 - accuracy: 0.7002\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 13s 170ms/step - loss: 0.6021 - accuracy: 0.7780\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.8487 - accuracy: 0.6536\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 14s 170ms/step - loss: 0.9256 - accuracy: 0.5920\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 13s 165ms/step - loss: 0.7739 - accuracy: 0.6850\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 13s 165ms/step - loss: 0.6261 - accuracy: 0.7650\n",
      "38/38 [==============================] - 3s 61ms/step - loss: 0.8106 - accuracy: 0.6505\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 19s 224ms/step - loss: 0.9671 - accuracy: 0.5640\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 17s 226ms/step - loss: 0.7581 - accuracy: 0.7002\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 17s 230ms/step - loss: 0.5153 - accuracy: 0.8123\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 0.8268 - accuracy: 0.6623\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 21s 259ms/step - loss: 0.9383 - accuracy: 0.5946\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 19s 246ms/step - loss: 0.7240 - accuracy: 0.7151\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 19s 245ms/step - loss: 0.4712 - accuracy: 0.8301\n",
      "38/38 [==============================] - 4s 92ms/step - loss: 0.8937 - accuracy: 0.6523\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 21s 249ms/step - loss: 0.9435 - accuracy: 0.5922\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 19s 251ms/step - loss: 0.7467 - accuracy: 0.7063\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 19s 246ms/step - loss: 0.5020 - accuracy: 0.8198\n",
      "38/38 [==============================] - 4s 92ms/step - loss: 0.8665 - accuracy: 0.6559\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 20s 239ms/step - loss: 0.9246 - accuracy: 0.5803\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 18s 240ms/step - loss: 0.7699 - accuracy: 0.6866\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 18s 237ms/step - loss: 0.6078 - accuracy: 0.7685\n",
      "38/38 [==============================] - 4s 91ms/step - loss: 0.8225 - accuracy: 0.6325\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 20s 237ms/step - loss: 0.9122 - accuracy: 0.5923\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 18s 237ms/step - loss: 0.7609 - accuracy: 0.6938\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 18s 241ms/step - loss: 0.5960 - accuracy: 0.7798\n",
      "38/38 [==============================] - 4s 92ms/step - loss: 0.8427 - accuracy: 0.6453\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 20s 239ms/step - loss: 0.9201 - accuracy: 0.5806\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 18s 236ms/step - loss: 0.7631 - accuracy: 0.6943\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 18s 240ms/step - loss: 0.6021 - accuracy: 0.7788\n",
      "38/38 [==============================] - 4s 92ms/step - loss: 0.8207 - accuracy: 0.6567\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 20s 238ms/step - loss: 0.9629 - accuracy: 0.5795\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 18s 237ms/step - loss: 0.7677 - accuracy: 0.6903\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 18s 235ms/step - loss: 0.5393 - accuracy: 0.8028\n",
      "38/38 [==============================] - 4s 87ms/step - loss: 0.8332 - accuracy: 0.6623\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 19s 237ms/step - loss: 0.9533 - accuracy: 0.5886\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 18s 241ms/step - loss: 0.7472 - accuracy: 0.7033\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 18s 231ms/step - loss: 0.5139 - accuracy: 0.8102\n",
      "38/38 [==============================] - 4s 89ms/step - loss: 0.8851 - accuracy: 0.6515\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 20s 240ms/step - loss: 0.9355 - accuracy: 0.5932\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 18s 240ms/step - loss: 0.7447 - accuracy: 0.7092\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 18s 238ms/step - loss: 0.5363 - accuracy: 0.8016\n",
      "38/38 [==============================] - 4s 89ms/step - loss: 0.8464 - accuracy: 0.6567\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 19s 227ms/step - loss: 0.9244 - accuracy: 0.5834\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 17s 225ms/step - loss: 0.7809 - accuracy: 0.6878\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 17s 228ms/step - loss: 0.6331 - accuracy: 0.7596\n",
      "38/38 [==============================] - 4s 89ms/step - loss: 0.7773 - accuracy: 0.6639\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 19s 221ms/step - loss: 0.9096 - accuracy: 0.5971\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 17s 224ms/step - loss: 0.7484 - accuracy: 0.6963\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 17s 225ms/step - loss: 0.5990 - accuracy: 0.7780\n",
      "38/38 [==============================] - 4s 89ms/step - loss: 0.8662 - accuracy: 0.6399\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 20s 228ms/step - loss: 0.9246 - accuracy: 0.5899\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 17s 227ms/step - loss: 0.7763 - accuracy: 0.6745\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 17s 226ms/step - loss: 0.6302 - accuracy: 0.7606\n",
      "38/38 [==============================] - 4s 90ms/step - loss: 0.8017 - accuracy: 0.6572\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 27s 335ms/step - loss: 0.9431 - accuracy: 0.5855\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 26s 344ms/step - loss: 0.7329 - accuracy: 0.7157\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 25s 329ms/step - loss: 0.4834 - accuracy: 0.8228\n",
      "38/38 [==============================] - 5s 121ms/step - loss: 0.8488 - accuracy: 0.6503\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 29s 332ms/step - loss: 0.9346 - accuracy: 0.5933\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 25s 329ms/step - loss: 0.7171 - accuracy: 0.7116\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 25s 328ms/step - loss: 0.4599 - accuracy: 0.8327\n",
      "38/38 [==============================] - 5s 124ms/step - loss: 0.9226 - accuracy: 0.6267\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 27s 329ms/step - loss: 0.9319 - accuracy: 0.5928\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 24s 320ms/step - loss: 0.7276 - accuracy: 0.7077\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 25s 333ms/step - loss: 0.4841 - accuracy: 0.8231\n",
      "38/38 [==============================] - 5s 125ms/step - loss: 0.8787 - accuracy: 0.6547\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 25s 307ms/step - loss: 0.9130 - accuracy: 0.5907\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 0.7531 - accuracy: 0.6998\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 25s 332ms/step - loss: 0.5912 - accuracy: 0.7782\n",
      "38/38 [==============================] - 5s 126ms/step - loss: 0.8026 - accuracy: 0.6623\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 27s 325ms/step - loss: 0.8993 - accuracy: 0.6002\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 24s 321ms/step - loss: 0.7263 - accuracy: 0.7137\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 25s 325ms/step - loss: 0.5634 - accuracy: 0.7941\n",
      "38/38 [==============================] - 5s 124ms/step - loss: 0.8604 - accuracy: 0.6594\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 32s 394ms/step - loss: 0.9083 - accuracy: 0.5922\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 28s 370ms/step - loss: 0.7437 - accuracy: 0.7026\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 25s 322ms/step - loss: 0.5846 - accuracy: 0.7799\n",
      "38/38 [==============================] - 5s 126ms/step - loss: 0.8329 - accuracy: 0.6510\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 42s 521ms/step - loss: 0.9387 - accuracy: 0.5832\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 40s 525ms/step - loss: 0.7167 - accuracy: 0.7195\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 40s 528ms/step - loss: 0.4585 - accuracy: 0.8303\n",
      "38/38 [==============================] - 7s 158ms/step - loss: 0.8904 - accuracy: 0.6589\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 43s 529ms/step - loss: 0.9211 - accuracy: 0.5935\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 39s 519ms/step - loss: 0.6870 - accuracy: 0.7350\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 39s 510ms/step - loss: 0.4333 - accuracy: 0.8441\n",
      "38/38 [==============================] - 7s 165ms/step - loss: 0.9342 - accuracy: 0.6403\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 41s 516ms/step - loss: 0.9278 - accuracy: 0.5949\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 39s 513ms/step - loss: 0.7076 - accuracy: 0.7214\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 39s 511ms/step - loss: 0.4436 - accuracy: 0.8452\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 0.9188 - accuracy: 0.6609\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 42s 519ms/step - loss: 0.9134 - accuracy: 0.5987\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 39s 507ms/step - loss: 0.7451 - accuracy: 0.7023\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 40s 521ms/step - loss: 0.5744 - accuracy: 0.7798\n",
      "38/38 [==============================] - 6s 154ms/step - loss: 0.8110 - accuracy: 0.6722\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 41s 516ms/step - loss: 0.8945 - accuracy: 0.5954\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 39s 518ms/step - loss: 0.7064 - accuracy: 0.7203\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 39s 509ms/step - loss: 0.5360 - accuracy: 0.8055\n",
      "38/38 [==============================] - 6s 146ms/step - loss: 0.8640 - accuracy: 0.6499\n",
      "Epoch 1/3\n",
      "76/76 [==============================] - 40s 502ms/step - loss: 0.9044 - accuracy: 0.5951\n",
      "Epoch 2/3\n",
      "76/76 [==============================] - 38s 502ms/step - loss: 0.7295 - accuracy: 0.7187\n",
      "Epoch 3/3\n",
      "76/76 [==============================] - 39s 511ms/step - loss: 0.5663 - accuracy: 0.7885\n",
      "38/38 [==============================] - 6s 155ms/step - loss: 0.8451 - accuracy: 0.6538\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 10s 108ms/step - loss: 1.0107 - accuracy: 0.5596\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 8s 108ms/step - loss: 0.8010 - accuracy: 0.6775\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 8s 104ms/step - loss: 0.6189 - accuracy: 0.7782\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 8s 105ms/step - loss: 0.4264 - accuracy: 0.8545\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 8s 105ms/step - loss: 0.2877 - accuracy: 0.9001\n",
      "38/38 [==============================] - 2s 32ms/step - loss: 0.9358 - accuracy: 0.6449\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 11s 104ms/step - loss: 0.9899 - accuracy: 0.5849\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 8s 104ms/step - loss: 0.8010 - accuracy: 0.6519\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 8s 104ms/step - loss: 0.6032 - accuracy: 0.7823\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 8s 104ms/step - loss: 0.4007 - accuracy: 0.8673\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 8s 109ms/step - loss: 0.2661 - accuracy: 0.9107\n",
      "38/38 [==============================] - 1s 27ms/step - loss: 0.9800 - accuracy: 0.6536\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 9s 102ms/step - loss: 0.9622 - accuracy: 0.5893\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 7s 96ms/step - loss: 0.7966 - accuracy: 0.6621\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 7s 97ms/step - loss: 0.6156 - accuracy: 0.7863\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 7s 96ms/step - loss: 0.4231 - accuracy: 0.8528\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 7s 96ms/step - loss: 0.2874 - accuracy: 0.8995\n",
      "38/38 [==============================] - 1s 28ms/step - loss: 0.9146 - accuracy: 0.6634\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 8s 87ms/step - loss: 0.9565 - accuracy: 0.5749\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 7s 86ms/step - loss: 0.8255 - accuracy: 0.6556\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 7s 87ms/step - loss: 0.7119 - accuracy: 0.7374\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 7s 86ms/step - loss: 0.6013 - accuracy: 0.7860\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 7s 87ms/step - loss: 0.5021 - accuracy: 0.8191\n",
      "38/38 [==============================] - 1s 27ms/step - loss: 0.8061 - accuracy: 0.6676\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 8s 85ms/step - loss: 0.9300 - accuracy: 0.5952\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 9s 119ms/step - loss: 0.8103 - accuracy: 0.6490\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 8s 101ms/step - loss: 0.6859 - accuracy: 0.7459\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 7s 95ms/step - loss: 0.5619 - accuracy: 0.7984\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 7s 97ms/step - loss: 0.4576 - accuracy: 0.8441\n",
      "38/38 [==============================] - 2s 29ms/step - loss: 0.8757 - accuracy: 0.6523\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 10s 95ms/step - loss: 0.9461 - accuracy: 0.5912\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 7s 96ms/step - loss: 0.7972 - accuracy: 0.6726\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 7s 95ms/step - loss: 0.6839 - accuracy: 0.7582\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 7s 96ms/step - loss: 0.5742 - accuracy: 0.8007\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 7s 97ms/step - loss: 0.4792 - accuracy: 0.8324\n",
      "38/38 [==============================] - 2s 29ms/step - loss: 0.8381 - accuracy: 0.6613\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 13s 139ms/step - loss: 0.9836 - accuracy: 0.5706\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 11s 144ms/step - loss: 0.8203 - accuracy: 0.6473\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 11s 140ms/step - loss: 0.6120 - accuracy: 0.7813\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 11s 146ms/step - loss: 0.4003 - accuracy: 0.8658\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 10s 136ms/step - loss: 0.2710 - accuracy: 0.9101\n",
      "38/38 [==============================] - 3s 53ms/step - loss: 0.9455 - accuracy: 0.6585\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 13s 147ms/step - loss: 0.9569 - accuracy: 0.5958\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 11s 146ms/step - loss: 0.7828 - accuracy: 0.6775\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 11s 150ms/step - loss: 0.5690 - accuracy: 0.8005\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 11s 147ms/step - loss: 0.3791 - accuracy: 0.8720\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 11s 147ms/step - loss: 0.2571 - accuracy: 0.9210\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 1.0305 - accuracy: 0.6304\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 13s 148ms/step - loss: 0.9535 - accuracy: 0.5909\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 11s 141ms/step - loss: 0.7761 - accuracy: 0.6813\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 10s 132ms/step - loss: 0.5698 - accuracy: 0.8005\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 10s 135ms/step - loss: 0.3754 - accuracy: 0.8766\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 0.2557 - accuracy: 0.9150\n",
      "38/38 [==============================] - 2s 41ms/step - loss: 0.9855 - accuracy: 0.6555\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 12s 128ms/step - loss: 0.9525 - accuracy: 0.5749\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 10s 131ms/step - loss: 0.8112 - accuracy: 0.6655\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 10s 127ms/step - loss: 0.6735 - accuracy: 0.7515\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 9s 122ms/step - loss: 0.5460 - accuracy: 0.8030\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 9s 121ms/step - loss: 0.4402 - accuracy: 0.8456\n",
      "38/38 [==============================] - 2s 43ms/step - loss: 0.8384 - accuracy: 0.6722\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 12s 129ms/step - loss: 0.9449 - accuracy: 0.5933\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 10s 129ms/step - loss: 0.7937 - accuracy: 0.6765\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 9s 124ms/step - loss: 0.6563 - accuracy: 0.7612\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 9s 121ms/step - loss: 0.5345 - accuracy: 0.8106\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 9s 120ms/step - loss: 0.4356 - accuracy: 0.8466\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.9043 - accuracy: 0.6577\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 12s 124ms/step - loss: 0.9194 - accuracy: 0.5920\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 10s 128ms/step - loss: 0.7922 - accuracy: 0.6709\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 10s 129ms/step - loss: 0.6592 - accuracy: 0.7592\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 9s 124ms/step - loss: 0.5370 - accuracy: 0.8055\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 10s 125ms/step - loss: 0.4372 - accuracy: 0.8499\n",
      "38/38 [==============================] - 2s 41ms/step - loss: 0.8683 - accuracy: 0.6476\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 14s 164ms/step - loss: 0.9597 - accuracy: 0.5849\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 12s 163ms/step - loss: 0.7832 - accuracy: 0.6907\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 13s 165ms/step - loss: 0.5617 - accuracy: 0.7953\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 12s 162ms/step - loss: 0.3580 - accuracy: 0.8817\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 12s 163ms/step - loss: 0.2444 - accuracy: 0.9115\n",
      "38/38 [==============================] - 3s 53ms/step - loss: 0.9742 - accuracy: 0.6606\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 14s 167ms/step - loss: 0.9566 - accuracy: 0.5741\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 0.7639 - accuracy: 0.6924\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 13s 164ms/step - loss: 0.5237 - accuracy: 0.8199\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 13s 166ms/step - loss: 0.3346 - accuracy: 0.8935\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 0.2270 - accuracy: 0.9274\n",
      "38/38 [==============================] - 4s 52ms/step - loss: 1.0702 - accuracy: 0.6267\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 15s 168ms/step - loss: 0.9538 - accuracy: 0.5819\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 0.7744 - accuracy: 0.6769\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 13s 169ms/step - loss: 0.5464 - accuracy: 0.8061\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 15s 194ms/step - loss: 0.3543 - accuracy: 0.8776\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 18s 236ms/step - loss: 0.2410 - accuracy: 0.9153\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 1.0263 - accuracy: 0.6505\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 20s 221ms/step - loss: 0.9275 - accuracy: 0.5855\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 16s 209ms/step - loss: 0.8036 - accuracy: 0.6756\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 13s 167ms/step - loss: 0.6603 - accuracy: 0.7550\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 12s 162ms/step - loss: 0.5283 - accuracy: 0.8065\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 12s 158ms/step - loss: 0.4244 - accuracy: 0.8551\n",
      "38/38 [==============================] - 3s 56ms/step - loss: 0.8436 - accuracy: 0.6565\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 19s 208ms/step - loss: 0.9311 - accuracy: 0.5878\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 16s 208ms/step - loss: 0.7779 - accuracy: 0.6895\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 16s 208ms/step - loss: 0.6305 - accuracy: 0.7660\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 16s 211ms/step - loss: 0.5039 - accuracy: 0.8179\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 16s 217ms/step - loss: 0.4007 - accuracy: 0.8679\n",
      "38/38 [==============================] - 4s 92ms/step - loss: 0.9312 - accuracy: 0.6453\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 20s 223ms/step - loss: 0.9283 - accuracy: 0.5866\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 17s 227ms/step - loss: 0.7791 - accuracy: 0.6796\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 12s 159ms/step - loss: 0.6382 - accuracy: 0.7673\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 12s 156ms/step - loss: 0.5152 - accuracy: 0.8193\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 13s 166ms/step - loss: 0.4132 - accuracy: 0.8576\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.8767 - accuracy: 0.6580\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 18s 198ms/step - loss: 0.9903 - accuracy: 0.5663\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 15s 202ms/step - loss: 0.7978 - accuracy: 0.6793\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 15s 202ms/step - loss: 0.5979 - accuracy: 0.7813\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 15s 198ms/step - loss: 0.3946 - accuracy: 0.8633\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 15s 194ms/step - loss: 0.2626 - accuracy: 0.9049\n",
      "38/38 [==============================] - 3s 57ms/step - loss: 1.0041 - accuracy: 0.6461\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 16s 180ms/step - loss: 0.9781 - accuracy: 0.5745\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 14s 183ms/step - loss: 0.7705 - accuracy: 0.6787\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 13s 175ms/step - loss: 0.5535 - accuracy: 0.8009\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 14s 178ms/step - loss: 0.3612 - accuracy: 0.8753\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 14s 179ms/step - loss: 0.2408 - accuracy: 0.9111\n",
      "38/38 [==============================] - 3s 65ms/step - loss: 1.0652 - accuracy: 0.6370\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 16s 178ms/step - loss: 0.9810 - accuracy: 0.5655\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 14s 178ms/step - loss: 0.7753 - accuracy: 0.6887\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 13s 174ms/step - loss: 0.5849 - accuracy: 0.7879\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 13s 176ms/step - loss: 0.3953 - accuracy: 0.8628\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 14s 185ms/step - loss: 0.2623 - accuracy: 0.9082\n",
      "38/38 [==============================] - 3s 56ms/step - loss: 1.0056 - accuracy: 0.6481\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 15s 165ms/step - loss: 0.9347 - accuracy: 0.5882\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 12s 164ms/step - loss: 0.8026 - accuracy: 0.6736\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 13s 167ms/step - loss: 0.6716 - accuracy: 0.7534\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 13s 167ms/step - loss: 0.5498 - accuracy: 0.8003\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 13s 170ms/step - loss: 0.4433 - accuracy: 0.8418\n",
      "38/38 [==============================] - 3s 58ms/step - loss: 0.8590 - accuracy: 0.6424\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 15s 166ms/step - loss: 0.9402 - accuracy: 0.5898\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 12s 162ms/step - loss: 0.7760 - accuracy: 0.6798\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 13s 166ms/step - loss: 0.6365 - accuracy: 0.7625\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 13s 173ms/step - loss: 0.5108 - accuracy: 0.8168\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 12s 164ms/step - loss: 0.4114 - accuracy: 0.8578\n",
      "38/38 [==============================] - 3s 54ms/step - loss: 0.9085 - accuracy: 0.6461\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 15s 164ms/step - loss: 0.9213 - accuracy: 0.5951\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 13s 168ms/step - loss: 0.7848 - accuracy: 0.6749\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 13s 174ms/step - loss: 0.6518 - accuracy: 0.7534\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 13s 166ms/step - loss: 0.5258 - accuracy: 0.8121\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 13s 164ms/step - loss: 0.4235 - accuracy: 0.8537\n",
      "38/38 [==============================] - 3s 57ms/step - loss: 0.8640 - accuracy: 0.6596\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 21s 249ms/step - loss: 0.9638 - accuracy: 0.5723\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 19s 254ms/step - loss: 0.7764 - accuracy: 0.6930\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 15s 196ms/step - loss: 0.5486 - accuracy: 0.8032\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 14s 184ms/step - loss: 0.3510 - accuracy: 0.8791\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 16s 213ms/step - loss: 0.2387 - accuracy: 0.9125\n",
      "38/38 [==============================] - 4s 90ms/step - loss: 0.9995 - accuracy: 0.6581\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 21s 254ms/step - loss: 0.9475 - accuracy: 0.5940\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 19s 245ms/step - loss: 0.7530 - accuracy: 0.7023\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 19s 253ms/step - loss: 0.5086 - accuracy: 0.8166\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 20s 256ms/step - loss: 0.3184 - accuracy: 0.8898\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 19s 248ms/step - loss: 0.2117 - accuracy: 0.9264\n",
      "38/38 [==============================] - 4s 87ms/step - loss: 1.1090 - accuracy: 0.6271\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 23s 252ms/step - loss: 0.9603 - accuracy: 0.5835\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 19s 248ms/step - loss: 0.7585 - accuracy: 0.6984\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 19s 250ms/step - loss: 0.5280 - accuracy: 0.8076\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 19s 248ms/step - loss: 0.3387 - accuracy: 0.8824\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 20s 258ms/step - loss: 0.2361 - accuracy: 0.9101\n",
      "38/38 [==============================] - 4s 87ms/step - loss: 1.0569 - accuracy: 0.6530\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 20s 233ms/step - loss: 0.9328 - accuracy: 0.5803\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 18s 238ms/step - loss: 0.7742 - accuracy: 0.6934\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 18s 234ms/step - loss: 0.6320 - accuracy: 0.7633\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 18s 242ms/step - loss: 0.5067 - accuracy: 0.8135\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 18s 240ms/step - loss: 0.4015 - accuracy: 0.8534\n",
      "38/38 [==============================] - 4s 90ms/step - loss: 0.8559 - accuracy: 0.6722\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 20s 236ms/step - loss: 0.9146 - accuracy: 0.5962\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 19s 243ms/step - loss: 0.7423 - accuracy: 0.7064\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 18s 242ms/step - loss: 0.5904 - accuracy: 0.7817\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 18s 237ms/step - loss: 0.4667 - accuracy: 0.8321\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 18s 238ms/step - loss: 0.3691 - accuracy: 0.8714\n",
      "38/38 [==============================] - 4s 88ms/step - loss: 0.9495 - accuracy: 0.6482\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 21s 236ms/step - loss: 0.9142 - accuracy: 0.5953\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 19s 247ms/step - loss: 0.7731 - accuracy: 0.6827\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 18s 238ms/step - loss: 0.6299 - accuracy: 0.7662\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 18s 241ms/step - loss: 0.5036 - accuracy: 0.8146\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 19s 243ms/step - loss: 0.4013 - accuracy: 0.8545\n",
      "38/38 [==============================] - 4s 96ms/step - loss: 0.8977 - accuracy: 0.6572\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 26s 307ms/step - loss: 0.9523 - accuracy: 0.5787\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 18s 230ms/step - loss: 0.7678 - accuracy: 0.6982\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 17s 227ms/step - loss: 0.5224 - accuracy: 0.8143\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 22s 288ms/step - loss: 0.3266 - accuracy: 0.8851\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 0.2312 - accuracy: 0.9125\n",
      "38/38 [==============================] - 5s 120ms/step - loss: 1.0075 - accuracy: 0.6660\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 26s 311ms/step - loss: 0.9333 - accuracy: 0.5962\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 0.7325 - accuracy: 0.7192\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 24s 310ms/step - loss: 0.4883 - accuracy: 0.8251\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 0.3009 - accuracy: 0.8979\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 24s 321ms/step - loss: 0.2017 - accuracy: 0.9285\n",
      "38/38 [==============================] - 5s 113ms/step - loss: 1.1182 - accuracy: 0.6350\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 27s 328ms/step - loss: 0.9285 - accuracy: 0.5930\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 24s 316ms/step - loss: 0.7393 - accuracy: 0.7073\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 0.4902 - accuracy: 0.8286\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 21s 280ms/step - loss: 0.3130 - accuracy: 0.8931\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 18s 230ms/step - loss: 0.2184 - accuracy: 0.9155\n",
      "38/38 [==============================] - 4s 84ms/step - loss: 1.0938 - accuracy: 0.6381\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 25s 299ms/step - loss: 0.9259 - accuracy: 0.5869\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 0.7668 - accuracy: 0.7023\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 20s 267ms/step - loss: 0.6100 - accuracy: 0.7765\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 0.4825 - accuracy: 0.8259\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 0.3796 - accuracy: 0.8638\n",
      "38/38 [==============================] - 5s 113ms/step - loss: 0.8826 - accuracy: 0.6577\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 25s 299ms/step - loss: 0.9090 - accuracy: 0.5969\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 24s 309ms/step - loss: 0.7587 - accuracy: 0.6973\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 0.5926 - accuracy: 0.7829\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 23s 302ms/step - loss: 0.4614 - accuracy: 0.8371\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 0.3605 - accuracy: 0.8733\n",
      "38/38 [==============================] - 5s 111ms/step - loss: 0.9697 - accuracy: 0.6358\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 26s 304ms/step - loss: 0.9110 - accuracy: 0.5926\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 17s 224ms/step - loss: 0.7614 - accuracy: 0.6949\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 17s 222ms/step - loss: 0.6120 - accuracy: 0.7722\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 19s 254ms/step - loss: 0.4856 - accuracy: 0.8286\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 23s 296ms/step - loss: 0.3821 - accuracy: 0.8667\n",
      "38/38 [==============================] - 5s 121ms/step - loss: 0.9284 - accuracy: 0.6485\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 25s 310ms/step - loss: 0.9638 - accuracy: 0.5826\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 0.7741 - accuracy: 0.6932\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 0.5537 - accuracy: 0.7939\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 22s 290ms/step - loss: 0.3493 - accuracy: 0.8751\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 23s 296ms/step - loss: 0.2322 - accuracy: 0.9140\n",
      "38/38 [==============================] - 5s 110ms/step - loss: 1.0550 - accuracy: 0.6561\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 25s 292ms/step - loss: 0.9398 - accuracy: 0.5915\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 22s 290ms/step - loss: 0.7376 - accuracy: 0.7011\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 0.5046 - accuracy: 0.8135\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 0.3156 - accuracy: 0.8941\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 22s 289ms/step - loss: 0.2159 - accuracy: 0.9183\n",
      "38/38 [==============================] - 5s 111ms/step - loss: 1.1510 - accuracy: 0.6288\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 26s 295ms/step - loss: 0.9582 - accuracy: 0.5804\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 0.7563 - accuracy: 0.6964\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 22s 291ms/step - loss: 0.5325 - accuracy: 0.8063\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 18s 241ms/step - loss: 0.3405 - accuracy: 0.8818\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 16s 214ms/step - loss: 0.2325 - accuracy: 0.9136\n",
      "38/38 [==============================] - 5s 110ms/step - loss: 1.1136 - accuracy: 0.6319\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 24s 282ms/step - loss: 0.9240 - accuracy: 0.5882\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 22s 283ms/step - loss: 0.7868 - accuracy: 0.6808\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 22s 284ms/step - loss: 0.6412 - accuracy: 0.7587\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 22s 290ms/step - loss: 0.5090 - accuracy: 0.8098\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 22s 288ms/step - loss: 0.4034 - accuracy: 0.8528\n",
      "38/38 [==============================] - 5s 111ms/step - loss: 0.8820 - accuracy: 0.6643\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 24s 286ms/step - loss: 0.9134 - accuracy: 0.5969\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 20s 266ms/step - loss: 0.7534 - accuracy: 0.6982\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 15s 202ms/step - loss: 0.6060 - accuracy: 0.7744\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 18s 229ms/step - loss: 0.4812 - accuracy: 0.8265\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 22s 283ms/step - loss: 0.3838 - accuracy: 0.8654\n",
      "38/38 [==============================] - 5s 109ms/step - loss: 0.9613 - accuracy: 0.6507\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 24s 280ms/step - loss: 0.9178 - accuracy: 0.5914\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 21s 278ms/step - loss: 0.7734 - accuracy: 0.6838\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 18s 229ms/step - loss: 0.6294 - accuracy: 0.7604\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 16s 208ms/step - loss: 0.5004 - accuracy: 0.8142\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 21s 277ms/step - loss: 0.3941 - accuracy: 0.8568\n",
      "38/38 [==============================] - 5s 109ms/step - loss: 0.8888 - accuracy: 0.6605\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 35s 426ms/step - loss: 0.9452 - accuracy: 0.5861\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 32s 423ms/step - loss: 0.7328 - accuracy: 0.7108\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 32s 425ms/step - loss: 0.4763 - accuracy: 0.8253\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 33s 428ms/step - loss: 0.2959 - accuracy: 0.8925\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 32s 424ms/step - loss: 0.2046 - accuracy: 0.9262\n",
      "38/38 [==============================] - 7s 158ms/step - loss: 1.0947 - accuracy: 0.6519\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 34s 419ms/step - loss: 0.9390 - accuracy: 0.5969\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 23s 304ms/step - loss: 0.7133 - accuracy: 0.7174\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 24s 308ms/step - loss: 0.4610 - accuracy: 0.8334\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 23s 302ms/step - loss: 0.2876 - accuracy: 0.8966\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 0.1948 - accuracy: 0.9256\n",
      "38/38 [==============================] - 5s 110ms/step - loss: 1.1741 - accuracy: 0.6333\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 25s 307ms/step - loss: 0.9406 - accuracy: 0.5876\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 24s 308ms/step - loss: 0.7250 - accuracy: 0.7193\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 0.4743 - accuracy: 0.8303\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 23s 304ms/step - loss: 0.3012 - accuracy: 0.8907\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 24s 317ms/step - loss: 0.2095 - accuracy: 0.9204\n",
      "38/38 [==============================] - 5s 119ms/step - loss: 1.1340 - accuracy: 0.6572\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 24s 299ms/step - loss: 0.9182 - accuracy: 0.5911\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 0.7569 - accuracy: 0.7025\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 0.6000 - accuracy: 0.7676\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 23s 304ms/step - loss: 0.4697 - accuracy: 0.8251\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 0.3664 - accuracy: 0.8706\n",
      "38/38 [==============================] - 5s 117ms/step - loss: 0.9125 - accuracy: 0.6643\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 24s 299ms/step - loss: 0.8989 - accuracy: 0.5977\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 0.7277 - accuracy: 0.7106\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 0.5656 - accuracy: 0.7914\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 0.4383 - accuracy: 0.8412\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 0.3467 - accuracy: 0.8755\n",
      "38/38 [==============================] - 5s 113ms/step - loss: 0.9994 - accuracy: 0.6536\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 24s 295ms/step - loss: 0.9050 - accuracy: 0.5971\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 0.7425 - accuracy: 0.7079\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 22s 291ms/step - loss: 0.5898 - accuracy: 0.7772\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 0.4657 - accuracy: 0.8322\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 0.3665 - accuracy: 0.8665\n",
      "38/38 [==============================] - 5s 117ms/step - loss: 0.9568 - accuracy: 0.6526\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 41s 524ms/step - loss: 0.9414 - accuracy: 0.5801\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 40s 530ms/step - loss: 0.7256 - accuracy: 0.7133\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 41s 533ms/step - loss: 0.4594 - accuracy: 0.8346\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 39s 517ms/step - loss: 0.2841 - accuracy: 0.8946\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 41s 536ms/step - loss: 0.1966 - accuracy: 0.9235\n",
      "38/38 [==============================] - 7s 160ms/step - loss: 1.0952 - accuracy: 0.6585\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 42s 533ms/step - loss: 0.9177 - accuracy: 0.5989\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 41s 537ms/step - loss: 0.6901 - accuracy: 0.7290\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 42s 546ms/step - loss: 0.4256 - accuracy: 0.8522\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 42s 558ms/step - loss: 0.2613 - accuracy: 0.9076\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 41s 533ms/step - loss: 0.1794 - accuracy: 0.9361\n",
      "38/38 [==============================] - 7s 163ms/step - loss: 1.1835 - accuracy: 0.6197\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 42s 521ms/step - loss: 0.9267 - accuracy: 0.5955\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.7040 - accuracy: 0.7257\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 40s 524ms/step - loss: 0.4460 - accuracy: 0.8444\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 40s 522ms/step - loss: 0.2842 - accuracy: 0.8991\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 40s 526ms/step - loss: 0.1948 - accuracy: 0.9254\n",
      "38/38 [==============================] - 6s 159ms/step - loss: 1.1859 - accuracy: 0.6538\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 43s 519ms/step - loss: 0.9119 - accuracy: 0.5919\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 42s 560ms/step - loss: 0.7433 - accuracy: 0.7095\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 54s 712ms/step - loss: 0.5804 - accuracy: 0.7811\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 53s 698ms/step - loss: 0.4507 - accuracy: 0.8381\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 53s 696ms/step - loss: 0.3460 - accuracy: 0.8782\n",
      "38/38 [==============================] - 9s 220ms/step - loss: 0.9668 - accuracy: 0.6197\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 54s 669ms/step - loss: 0.8983 - accuracy: 0.6057\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 43s 559ms/step - loss: 0.7034 - accuracy: 0.7209\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 49s 641ms/step - loss: 0.5329 - accuracy: 0.8040\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 49s 641ms/step - loss: 0.4073 - accuracy: 0.8555\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 49s 642ms/step - loss: 0.3158 - accuracy: 0.8877\n",
      "38/38 [==============================] - 8s 200ms/step - loss: 1.0362 - accuracy: 0.6180\n",
      "Epoch 1/5\n",
      "76/76 [==============================] - 49s 609ms/step - loss: 0.9013 - accuracy: 0.5903\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 42s 547ms/step - loss: 0.7285 - accuracy: 0.7148\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 48s 635ms/step - loss: 0.5655 - accuracy: 0.7896\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 48s 638ms/step - loss: 0.4360 - accuracy: 0.8470\n",
      "Epoch 5/5\n",
      "76/76 [==============================] - 48s 634ms/step - loss: 0.3380 - accuracy: 0.8814\n",
      "38/38 [==============================] - 9s 225ms/step - loss: 0.9726 - accuracy: 0.6572\n",
      "Epoch 1/3\n",
      "454/454 [==============================] - 44s 90ms/step - loss: 0.8624 - accuracy: 0.6167\n",
      "Epoch 2/3\n",
      "454/454 [==============================] - 41s 91ms/step - loss: 0.6825 - accuracy: 0.7282\n",
      "Epoch 3/3\n",
      "454/454 [==============================] - 48s 105ms/step - loss: 0.5409 - accuracy: 0.7985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train_padded, y_train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.662348 using {'batch_size': 16, 'epochs': 3, 'filters': 128, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.657112 (0.005664) with: {'batch_size': 16, 'epochs': 3, 'filters': 64, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.653668 (0.015856) with: {'batch_size': 16, 'epochs': 3, 'filters': 64, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.653667 (0.011200) with: {'batch_size': 16, 'epochs': 3, 'filters': 64, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.657662 (0.008242) with: {'batch_size': 16, 'epochs': 3, 'filters': 64, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.643331 (0.006656) with: {'batch_size': 16, 'epochs': 3, 'filters': 64, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.661109 (0.012296) with: {'batch_size': 16, 'epochs': 3, 'filters': 64, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.653115 (0.004365) with: {'batch_size': 16, 'epochs': 3, 'filters': 128, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.657802 (0.009712) with: {'batch_size': 16, 'epochs': 3, 'filters': 128, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.639746 (0.011985) with: {'batch_size': 16, 'epochs': 3, 'filters': 128, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.656836 (0.005236) with: {'batch_size': 16, 'epochs': 3, 'filters': 128, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.649807 (0.006757) with: {'batch_size': 16, 'epochs': 3, 'filters': 128, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.662348 (0.004412) with: {'batch_size': 16, 'epochs': 3, 'filters': 128, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.650497 (0.009934) with: {'batch_size': 16, 'epochs': 3, 'filters': 256, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.654078 (0.009386) with: {'batch_size': 16, 'epochs': 3, 'filters': 256, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.639058 (0.003455) with: {'batch_size': 16, 'epochs': 3, 'filters': 256, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.653940 (0.012877) with: {'batch_size': 16, 'epochs': 3, 'filters': 256, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.648016 (0.012546) with: {'batch_size': 16, 'epochs': 3, 'filters': 256, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.648845 (0.018476) with: {'batch_size': 16, 'epochs': 3, 'filters': 256, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.647465 (0.005048) with: {'batch_size': 16, 'epochs': 5, 'filters': 64, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.653526 (0.012716) with: {'batch_size': 16, 'epochs': 5, 'filters': 64, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.641953 (0.015972) with: {'batch_size': 16, 'epochs': 5, 'filters': 64, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.649808 (0.020255) with: {'batch_size': 16, 'epochs': 5, 'filters': 64, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.641675 (0.004878) with: {'batch_size': 16, 'epochs': 5, 'filters': 64, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.647877 (0.006076) with: {'batch_size': 16, 'epochs': 5, 'filters': 64, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.642504 (0.010970) with: {'batch_size': 16, 'epochs': 5, 'filters': 128, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.652564 (0.003559) with: {'batch_size': 16, 'epochs': 5, 'filters': 128, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.641538 (0.012063) with: {'batch_size': 16, 'epochs': 5, 'filters': 128, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.652977 (0.008625) with: {'batch_size': 16, 'epochs': 5, 'filters': 128, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.642642 (0.011439) with: {'batch_size': 16, 'epochs': 5, 'filters': 128, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.658765 (0.005070) with: {'batch_size': 16, 'epochs': 5, 'filters': 128, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.645122 (0.010511) with: {'batch_size': 16, 'epochs': 5, 'filters': 256, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.631889 (0.018668) with: {'batch_size': 16, 'epochs': 5, 'filters': 256, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.634374 (0.012981) with: {'batch_size': 16, 'epochs': 5, 'filters': 256, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.627207 (0.041573) with: {'batch_size': 16, 'epochs': 5, 'filters': 256, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.642089 (0.006093) with: {'batch_size': 16, 'epochs': 5, 'filters': 256, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.628997 (0.027516) with: {'batch_size': 16, 'epochs': 5, 'filters': 256, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.659730 (0.002714) with: {'batch_size': 32, 'epochs': 3, 'filters': 64, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.656699 (0.010204) with: {'batch_size': 32, 'epochs': 3, 'filters': 64, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.649395 (0.012568) with: {'batch_size': 32, 'epochs': 3, 'filters': 64, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.647328 (0.008067) with: {'batch_size': 32, 'epochs': 3, 'filters': 64, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.644294 (0.003516) with: {'batch_size': 32, 'epochs': 3, 'filters': 64, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.659591 (0.008381) with: {'batch_size': 32, 'epochs': 3, 'filters': 64, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.661384 (0.005550) with: {'batch_size': 32, 'epochs': 3, 'filters': 128, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.644981 (0.013375) with: {'batch_size': 32, 'epochs': 3, 'filters': 128, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.646225 (0.009428) with: {'batch_size': 32, 'epochs': 3, 'filters': 128, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.643877 (0.018928) with: {'batch_size': 32, 'epochs': 3, 'filters': 128, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.651875 (0.005182) with: {'batch_size': 32, 'epochs': 3, 'filters': 128, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.647050 (0.004522) with: {'batch_size': 32, 'epochs': 3, 'filters': 128, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.642227 (0.013050) with: {'batch_size': 32, 'epochs': 3, 'filters': 256, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.617012 (0.046000) with: {'batch_size': 32, 'epochs': 3, 'filters': 256, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.641537 (0.004047) with: {'batch_size': 32, 'epochs': 3, 'filters': 256, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.651046 (0.007472) with: {'batch_size': 32, 'epochs': 3, 'filters': 256, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.648292 (0.008836) with: {'batch_size': 32, 'epochs': 3, 'filters': 256, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.645121 (0.001331) with: {'batch_size': 32, 'epochs': 3, 'filters': 256, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.649118 (0.003040) with: {'batch_size': 32, 'epochs': 5, 'filters': 64, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.659317 (0.007863) with: {'batch_size': 32, 'epochs': 5, 'filters': 64, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.648154 (0.006397) with: {'batch_size': 32, 'epochs': 5, 'filters': 64, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.648014 (0.009566) with: {'batch_size': 32, 'epochs': 5, 'filters': 64, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.646087 (0.008156) with: {'batch_size': 32, 'epochs': 5, 'filters': 64, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.657939 (0.013452) with: {'batch_size': 32, 'epochs': 5, 'filters': 64, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.650636 (0.008413) with: {'batch_size': 32, 'epochs': 5, 'filters': 128, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.651047 (0.002701) with: {'batch_size': 32, 'epochs': 5, 'filters': 128, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.638781 (0.011074) with: {'batch_size': 32, 'epochs': 5, 'filters': 128, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.641811 (0.014070) with: {'batch_size': 32, 'epochs': 5, 'filters': 128, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.646226 (0.012985) with: {'batch_size': 32, 'epochs': 5, 'filters': 128, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.640438 (0.013275) with: {'batch_size': 32, 'epochs': 5, 'filters': 128, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.646499 (0.004173) with: {'batch_size': 32, 'epochs': 5, 'filters': 256, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.658077 (0.016367) with: {'batch_size': 32, 'epochs': 5, 'filters': 256, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.639883 (0.006819) with: {'batch_size': 32, 'epochs': 5, 'filters': 256, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.639468 (0.014021) with: {'batch_size': 32, 'epochs': 5, 'filters': 256, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.644156 (0.004663) with: {'batch_size': 32, 'epochs': 5, 'filters': 256, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.656285 (0.017849) with: {'batch_size': 32, 'epochs': 5, 'filters': 256, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.661659 (0.011508) with: {'batch_size': 64, 'epochs': 3, 'filters': 64, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.650220 (0.004361) with: {'batch_size': 64, 'epochs': 3, 'filters': 64, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.657525 (0.009025) with: {'batch_size': 64, 'epochs': 3, 'filters': 64, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.657938 (0.003055) with: {'batch_size': 64, 'epochs': 3, 'filters': 64, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.648842 (0.004956) with: {'batch_size': 64, 'epochs': 3, 'filters': 64, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.651461 (0.005628) with: {'batch_size': 64, 'epochs': 3, 'filters': 64, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.654216 (0.009786) with: {'batch_size': 64, 'epochs': 3, 'filters': 128, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.659868 (0.005420) with: {'batch_size': 64, 'epochs': 3, 'filters': 128, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.658213 (0.009533) with: {'batch_size': 64, 'epochs': 3, 'filters': 128, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.657799 (0.008216) with: {'batch_size': 64, 'epochs': 3, 'filters': 128, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.656836 (0.004103) with: {'batch_size': 64, 'epochs': 3, 'filters': 128, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.644847 (0.009905) with: {'batch_size': 64, 'epochs': 3, 'filters': 128, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.656836 (0.004388) with: {'batch_size': 64, 'epochs': 3, 'filters': 256, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.653666 (0.010094) with: {'batch_size': 64, 'epochs': 3, 'filters': 256, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.643882 (0.012278) with: {'batch_size': 64, 'epochs': 3, 'filters': 256, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.657524 (0.004795) with: {'batch_size': 64, 'epochs': 3, 'filters': 256, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.653391 (0.009257) with: {'batch_size': 64, 'epochs': 3, 'filters': 256, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.658627 (0.009720) with: {'batch_size': 64, 'epochs': 3, 'filters': 256, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.653943 (0.007542) with: {'batch_size': 64, 'epochs': 5, 'filters': 64, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.660419 (0.006275) with: {'batch_size': 64, 'epochs': 5, 'filters': 64, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.648154 (0.012597) with: {'batch_size': 64, 'epochs': 5, 'filters': 64, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.659177 (0.010070) with: {'batch_size': 64, 'epochs': 5, 'filters': 64, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.645949 (0.014214) with: {'batch_size': 64, 'epochs': 5, 'filters': 64, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.653253 (0.005652) with: {'batch_size': 64, 'epochs': 5, 'filters': 64, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.643744 (0.004805) with: {'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.649395 (0.007399) with: {'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.646087 (0.013574) with: {'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.659178 (0.009893) with: {'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.646360 (0.013931) with: {'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.647327 (0.008981) with: {'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n",
      "0.638919 (0.012186) with: {'batch_size': 64, 'epochs': 5, 'filters': 256, 'kernel_size': 3, 'optimizer': 'adam'}\n",
      "0.658490 (0.005741) with: {'batch_size': 64, 'epochs': 5, 'filters': 256, 'kernel_size': 3, 'optimizer': 'rmsprop'}\n",
      "0.647466 (0.010228) with: {'batch_size': 64, 'epochs': 5, 'filters': 256, 'kernel_size': 5, 'optimizer': 'adam'}\n",
      "0.656835 (0.005310) with: {'batch_size': 64, 'epochs': 5, 'filters': 256, 'kernel_size': 5, 'optimizer': 'rmsprop'}\n",
      "0.644020 (0.017319) with: {'batch_size': 64, 'epochs': 5, 'filters': 256, 'kernel_size': 7, 'optimizer': 'adam'}\n",
      "0.631619 (0.018069) with: {'batch_size': 64, 'epochs': 5, 'filters': 256, 'kernel_size': 7, 'optimizer': 'rmsprop'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. **Significance of Sentiment Analysis**: At Nyika Analytika, we recognize the transformative power of sentiment analysis, particularly in today's digital age. Platforms such as Twitter serve as reservoirs of public sentiment, capturing the zeitgeist of the digital populace in real-time. Tapping into these vast repositories can furnish us with a deep understanding of public reactions, a knowledge invaluable to esteemed bodies like the Kenyan Parliamentary ICT Committee. Our dedicated utilization of state-of-the-art Natural Language Processing (NLP) techniques ensures that our methodologies are at the forefront of data science.\n",
    "\n",
    "2. **Dataset Inference**: The dataset we've meticulously sourced encompasses tweets targeting two of the world's technological behemoths: Apple and Google. Our initial exploration divulged variations in sentiment distributions and highlighted particular brand/product mentions. Furthermore, the presence of missing values in the dataset has accentuated the necessity for a rigorous and thorough data cleansing process.\n",
    "\n",
    "3. **Performance Metrics of Our Models**:\n",
    "   - **Logistic Regression**: Our meticulously trained model achieved an accuracy rate of 65%. It showcased commendable prowess in categorizing Neutral sentiments, reflecting an F1-score of 0.76. This indicates a harmonious balance between precision and recall for this specific category. Nevertheless, we acknowledge room for improvement in the categories of Negative and Unknown sentiments.\n",
    "   - **Random Forest**: This model, another jewel in our analytical arsenal, registered an accuracy of 63%. Echoing the Logistic Regression model, it exhibited a pronounced affinity for the Neutral sentiments with an F1-score of 0.75. We are actively seeking to enhance its efficacy for other sentiment categories.\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Data Cleansing and Refinement**: \n",
    "   - We advocate for the judicious handling of missing values. Their importance will dictate whether imputation or row deletion is the optimal strategy.\n",
    "   - We further recommend the incorporation of advanced text pre-processing techniques, encompassing stemming, lemmatization, and the elimination of special characters, to elevate the caliber of data being processed.\n",
    "\n",
    "2. **Innovative Feature Engineering**:\n",
    "   - We suggest delving into avant-garde text vectorization methodologies like TF-IDF, Word2Vec, or Doc2Vec. Their ability to encapsulate semantic word relationships can be the linchpin to enhancing model performance.\n",
    "   - Consider the extraction of nuanced features from tweets, such as tweet length, URL presence, or specific hashtag usage, as they might wield untapped predictive prowess.\n",
    "\n",
    "3. **Exploring Model Horizons**:\n",
    "   - We propose experimentation with intricate architectures like Neural Networks and ensemble methods. Models imbued with Deep Learning capabilities, such as LSTM or GRU, are renowned for their efficacy in sentiment analysis tasks, given their prowess in discerning textual sequences.\n",
    "   - We also recommend a rigorous phase of hyperparameter fine-tuning for our current models, ensuring they operate at their zenith.\n",
    "\n",
    "4. **Addressing Class Imbalance**:\n",
    "   - Our introspection has revealed potential class imbalances. To rectify this, we suggest techniques like oversampling, undersampling, or even the adoption of synthetic data methodologies like SMOTE.\n",
    "\n",
    "5. **Augmenting Data Sources**:\n",
    "   - In our quest for a holistic sentiment understanding, we're considering the integration of data from diverse social media platforms and tech forums. This multidimensional approach promises a more comprehensive sentiment landscape.\n",
    "\n",
    "6. **Iterative Feedback Mechanism**:\n",
    "   - We're in the process of instituting a feedback loop where our model's predictions undergo validation against real-world outcomes. This iterative process promises continuous model refinement.\n",
    "\n",
    "7. **Engaging with Stakeholders**:\n",
    "   - We underscore the importance of continuous dialogue with the Kenyan Parliamentary ICT Committee. This ensures our analytical endeavors remain aligned with their evolving requirements, guaranteeing actionable and pertinent insights.\n",
    "\n",
    "At Nyika Analytika, we believe in the transformative potential of sentiment analysis. Our relentless quest for perfection, combined with the above recommendations, aims to empower Kenya's Parliamentary ICT Committee, aiding them in sculpting a future aligned with public sentiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
